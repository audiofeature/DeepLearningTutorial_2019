{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music vs. Speech Classification with Deep Learning\n",
    "\n",
    "This tutorial shows how different Convolutional Neural Network architectures are used for the task of discriminating a piece of audio whether it is music or speech (binary classification).\n",
    "\n",
    "The data set used is the [Music Speech](http://marsyasweb.appspot.com/download/data_sets/) data set compiled by George Tzanetakis. It consists of 128 tracks, each 30 seconds long. Each class (music/speech) has 64 examples. The tracks are all 22050Hz Mono 16-bit audio files in .wav format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "This tutorial contains:\n",
    "* Loading and Preprocessing of Audio files\n",
    "* Loading class files from CSV and using Label Encoder\n",
    "* Audio Preprocessing: Generating log Mel spectrograms\n",
    "* Standardization of Data\n",
    "* Convolutional Neural Networks: single, stacked, parallel\n",
    "* ReLU Activation\n",
    "* Dropout\n",
    "* Train/Test set split\n",
    "\n",
    "You can execute the following code blocks by pressing SHIFT+Enter consecutively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "The data set can be downloaded from [here](http://opihi.cs.uvic.ca/sound/music_speech.tar.gz).\n",
    "\n",
    "Please unpack it (it is .tar.gz compressed)\n",
    "\n",
    "Set the path to the unpacked folder in the next box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH  = '/home/schindler/tutorials/mlprague2018/data/gtzan_music_speech'\n",
    "AUDIO_PATH = DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schindler/anaconda/python2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd # Pandas for reading CSV files and easier Data handling in preparation\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports for audio reading and processing\n",
    "import audio_spectrogram as rp\n",
    "from audiofile_read import audiofile_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Metadata\n",
    "\n",
    "The tab-separated file contains pairs of filename TAB class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>speech/stupid.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/teachers2.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/danie1.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/oneday.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/jvoice.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/relation.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/geography.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/pulp2.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/greek1.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/conversion.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            1\n",
       "0                            \n",
       "speech/stupid.wav      speech\n",
       "speech/teachers2.wav   speech\n",
       "speech/danie1.wav      speech\n",
       "speech/oneday.wav      speech\n",
       "speech/jvoice.wav      speech\n",
       "speech/relation.wav    speech\n",
       "speech/geography.wav   speech\n",
       "speech/pulp2.wav       speech\n",
       "speech/greek1.wav      speech\n",
       "speech/conversion.wav  speech"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = os.path.join(DATA_PATH,'filelist_wav_relpath_wclasses.txt')\n",
    "metadata = pd.read_csv(csv_file, index_col=0, sep='\\t', header=None)\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two lists: one with filenames and one with associated classes\n",
    "filelist = metadata.index.tolist()\n",
    "classes = metadata[1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Labels to Numbers\n",
    "\n",
    "String labels need to be encoded as numbers. We use the LabelEncoder from the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speech', 'speech', 'speech', 'speech', 'speech']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 classes: music, speech\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(classes)\n",
    "print(len(labelencoder.classes_), \"classes:\", \", \".join(list(labelencoder.classes_)))\n",
    "\n",
    "classes_num = labelencoder.transform(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first few classes (speech)\n",
    "classes_num[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first few classes (music)\n",
    "classes_num[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In order to correctly re-transform any predicted numbers into strings, we keep the labelencoder for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................\n",
      "Read 128 audio files\n"
     ]
    }
   ],
   "source": [
    "list_spectrograms = [] # spectrograms are put into a list first\n",
    "\n",
    "# desired output parameters\n",
    "n_mel_bands = 40   # y axis\n",
    "frames = 80        # x axis\n",
    "\n",
    "# some FFT parameters\n",
    "fft_window_size=512\n",
    "fft_overlap = 0.5\n",
    "hop_size = int(fft_window_size*(1-fft_overlap))\n",
    "segment_size = fft_window_size + (frames-1) * hop_size # segment size for desired # frames\n",
    "\n",
    "for filename in filelist:\n",
    "    print(\".\", end='')\n",
    "    filepath = os.path.join(AUDIO_PATH, filename)\n",
    "    samplerate, samplewidth, wavedata = audiofile_read(filepath,verbose=False)\n",
    "    sample_length = wavedata.shape[0]\n",
    "\n",
    "    # make Mono (in case of multiple channels / stereo)\n",
    "    if wavedata.ndim > 1:\n",
    "        wavedata = np.mean(wavedata, 1)\n",
    "        \n",
    "    # take only a segment; choose start position:\n",
    "    #pos = 0 # beginning\n",
    "    pos = int(wavedata.shape[0]/2 - segment_size/2) # center minus half segment size\n",
    "    wav_segment = wavedata[pos:pos+segment_size]\n",
    "\n",
    "    # 1) FFT spectrogram \n",
    "    spectrogram = rp.calc_spectrogram(wav_segment,fft_window_size,fft_overlap)\n",
    "\n",
    "    # 2) Transform to perceptual Mel scale (uses librosa.filters.mel)\n",
    "    spectrogram = rp.transform2mel(spectrogram,samplerate,fft_window_size,n_mel_bands)\n",
    "        \n",
    "    # 3) Log 10 transform\n",
    "    spectrogram = np.log10(spectrogram)\n",
    "    \n",
    "    list_spectrograms.append(spectrogram)\n",
    "        \n",
    "print(\"\\nRead\", len(filelist), \"audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00320435, -0.05340576, -0.09249878, ...,  0.14624023,\n",
       "        0.19299316,  0.18325806])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 80)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An audio segment is 0.94 seconds long\n"
     ]
    }
   ],
   "source": [
    "print(\"An audio segment is\", round(float(segment_size) / samplerate, 2), \"seconds long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For simplicity of this tutorial, here we load only 1 single segment of ~ 1 second length from each audio file.\n",
    "In a real setting, one would create training instances of as many audio segments as possible to be fed to a Neural Network.\n",
    "\n",
    "### Show Waveform and Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'music/echoes.wav'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48d038d5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you can skip this if you do not have matplotlib installed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# show 1 sec wave segment\n",
    "plt.plot(wav_segment)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48d038d470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show spectrogram\n",
    "fig = plt.imshow(spectrogram, origin='lower')\n",
    "fig.set_cmap('jet')\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 1 big array of list of spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the input data to the right data type used by Keras Deep Learning (GPU)\n",
    "dtype = keras.backend.floatx()\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 40, 80)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of many 40x80 spectrograms is made into 1 big array\n",
    "data = np.array(list_spectrograms, dtype=dtype)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "We use <b>Zero-mean Unit-variance standardization</b> (also known as Z-score normalization).\n",
    "Here, we use <b>attribute-wise standardization</b>, i.e. each pixel is standardized individually, as opposed to computing a single mean and single standard deviation of all values.\n",
    "\n",
    "('Flat' standardization would also be possible, but we have seen benefits of attribut-wise standardization in our experiments).\n",
    "\n",
    "We use the StandardScaler from the scikit-learn package for our purpose.\n",
    "As it works typically on vector data, we have to vectorize (i.e. reshape) our matrices first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 3200)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize\n",
    "N, ydim, xdim = data.shape\n",
    "data = data.reshape(N, xdim*ydim)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "scaler = preprocessing.StandardScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.983009 , -3.997586 , -3.9986486, ..., -8.036928 , -8.029473 ,\n",
       "        -8.038338 ], dtype=float32),\n",
       " array([1.0415441, 1.0475882, 1.0341036, ..., 1.0074264, 0.9896529,\n",
       "        1.0046793], dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show mean and standard deviation: two vectors with same length as data.shape[1]\n",
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train & Test Set \n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%).\n",
    "\n",
    "Here we compare Random Split vs. Stratified Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_size = 0.25 # % portion of whole data set to keep for testing, i.e. 75% is used for training\n",
    "\n",
    "# Normal (random) split of data set into 2 parts\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set, train_classes, test_classes = train_test_split(data, classes_num, test_size=testset_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: Class 0: 49 Class 1: 47\n"
     ]
    }
   ],
   "source": [
    "# The two classes may be unbalanced\n",
    "print(\"Class Counts: Class 0:\", sum(train_classes==0), \"Class 1:\", sum(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN INDEX: [ 97 102   0  34  13  44  60  20  75  22   6  24  99 116  47  61  14 126\n",
      "  68 104  89  17  95   7  37   3  93   8  40   9  69 122  72 119  71  82\n",
      "  33  74  51  49  78  29  76  98   5  50 121  23  12 105  36  96  25  90\n",
      " 101  55  92  21  80   2 125  16  32 127 124  48  28 112  63 110  77 109\n",
      "  54  79  45  53  66 115  86 120  52  59  81 118 106  19  43  94  65  15\n",
      "  26  84  30 107  10  91]\n",
      "TEST INDEX: [  1  27  64 117  88  85  35  18  46 100 111  11  83 103  87   4  70  31\n",
      " 108  58  57 123  38  73  56 113  67  39 114  42  41  62]\n"
     ]
    }
   ],
   "source": [
    "# better: Stratified Split retains the class balance in both sets\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(data, classes_num)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes_num[train_index]\n",
    "    test_classes = classes_num[test_index]\n",
    "# Note: this for loop is only executed once, if n_iter==1 resp. n_splits==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 3200)\n",
      "(32, 3200)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "# Note: we will reshape the data later back to matrix form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: Class 0: 48 Class 1: 48\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Counts: Class 0:\", sum(train_classes==0), \"Class 1:\", sum(train_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images or spectrograms, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which aggregates neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "### Adding the channel\n",
    "\n",
    "As CNNs were initially made for image data, we need to add a dimension for the color channel to the data. RGB images typically have a 3rd dimension with the color. \n",
    "\n",
    "<b>Spectrograms, however, are considered like greyscale images, as in the previous tutorial.\n",
    "Likewise we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3).\n",
    "\n",
    "<i>Note on Tensorflow vs. Theano:</i>\n",
    "\n",
    "In Theano, traditionally the color channel was the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" (for Theano or Tensorflow) *or* with \"image_data_format\" set to \"channels_first\" or \"channels_last\".\n",
    "\n",
    "Tensorflow is now the default image ordering for Kears (\"tf\" and/or \"channels_last\").\n",
    "To be on the safe side, we added the if statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " keras.backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1 # for grey-scale, 3 for RGB, but usually already present in the data\n",
    "\n",
    "if keras.backend.image_data_format() == 'channels_last':  # TENSORFLOW\n",
    "    # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "    train_set = train_set.reshape(train_set.shape[0], ydim, xdim, n_channels)\n",
    "    test_set = test_set.reshape(test_set.shape[0], ydim, xdim, n_channels)\n",
    "else: # THEANO\n",
    "    # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "    train_set = train_set.reshape(train_set.shape[0], n_channels, ydim, xdim)\n",
    "    test_set = test_set.reshape(test_set.shape[0], n_channels, ydim, xdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 40, 80, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 40, 80, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 80, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "input_shape = train_set.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network Models in Keras\n",
    "\n",
    "## Sequential Models\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Single Layer and a Two Layer CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try: (comment/uncomment code in the following code block)\n",
    "* 1 Layer\n",
    "* 2 Layer\n",
    "* more conv_filters\n",
    "* Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(0) # make results repeatable\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "conv_filters = 32   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, (3, 3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, (3, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256, activation='sigmoid')) \n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 38, 78, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 39, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 39, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 37, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 18, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,189,729\n",
      "Trainable params: 1,189,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.6948 - acc: 0.5000\n",
      "Epoch 2/15\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.6634 - acc: 0.6250\n",
      "Epoch 3/15\n",
      "96/96 [==============================] - 0s 527us/step - loss: 0.6418 - acc: 0.7292\n",
      "Epoch 4/15\n",
      "96/96 [==============================] - 0s 219us/step - loss: 0.6185 - acc: 0.8333\n",
      "Epoch 5/15\n",
      "96/96 [==============================] - 0s 224us/step - loss: 0.5949 - acc: 0.7917\n",
      "Epoch 6/15\n",
      "96/96 [==============================] - 0s 217us/step - loss: 0.5690 - acc: 0.8229\n",
      "Epoch 7/15\n",
      "96/96 [==============================] - 0s 198us/step - loss: 0.5543 - acc: 0.8333\n",
      "Epoch 8/15\n",
      "96/96 [==============================] - 0s 203us/step - loss: 0.5364 - acc: 0.8229\n",
      "Epoch 9/15\n",
      "96/96 [==============================] - 0s 195us/step - loss: 0.5192 - acc: 0.8021\n",
      "Epoch 10/15\n",
      "96/96 [==============================] - 0s 202us/step - loss: 0.5068 - acc: 0.8021\n",
      "Epoch 11/15\n",
      "96/96 [==============================] - 0s 196us/step - loss: 0.4954 - acc: 0.8125\n",
      "Epoch 12/15\n",
      "96/96 [==============================] - 0s 209us/step - loss: 0.4771 - acc: 0.8021\n",
      "Epoch 13/15\n",
      "96/96 [==============================] - 0s 213us/step - loss: 0.4686 - acc: 0.8021\n",
      "Epoch 14/15\n",
      "96/96 [==============================] - 0s 204us/step - loss: 0.4611 - acc: 0.8125\n",
      "Epoch 15/15\n",
      "96/96 [==============================] - 0s 210us/step - loss: 0.4482 - acc: 0.8021\n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy goes up pretty quickly for 1 layer on Train set! Also on Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# always execute this, and then a box of accuracy_score below to print the result\n",
    "test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 layer\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layer\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layer + 32 convolution filters\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layer + 32 convolution filters + Dropout\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Parameters & Techniques\n",
    "\n",
    "Try out more parameters and techniques: (comment/uncomment code blocks below)\n",
    "* Adding ReLU activation\n",
    "* Adding Batch normalization\n",
    "* Adding Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "filter_size = (3,3)\n",
    "pool_size = (2,2)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, filter_size, padding='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=pool_size)) \n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, filter_size, padding='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=pool_size)) \n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256))  \n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIREX 2015 model\n",
    "model = Sequential()\n",
    "\n",
    "conv_filters = 15   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, (12, 8), padding='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(Activation('sigmoid')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 1))) \n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(200, activation='sigmoid'))  \n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and training the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.7584 - acc: 0.5521\n",
      "Epoch 2/15\n",
      "96/96 [==============================] - 0s 209us/step - loss: 0.7902 - acc: 0.4792\n",
      "Epoch 3/15\n",
      "96/96 [==============================] - 0s 203us/step - loss: 0.7644 - acc: 0.5417\n",
      "Epoch 4/15\n",
      "96/96 [==============================] - 0s 207us/step - loss: 0.7855 - acc: 0.5208\n",
      "Epoch 5/15\n",
      "96/96 [==============================] - 0s 275us/step - loss: 0.6474 - acc: 0.6042\n",
      "Epoch 6/15\n",
      "96/96 [==============================] - 0s 198us/step - loss: 0.6623 - acc: 0.5938\n",
      "Epoch 7/15\n",
      "96/96 [==============================] - 0s 199us/step - loss: 0.6526 - acc: 0.6146\n",
      "Epoch 8/15\n",
      "96/96 [==============================] - 0s 194us/step - loss: 0.6275 - acc: 0.6562\n",
      "Epoch 9/15\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.6688 - acc: 0.6250\n",
      "Epoch 10/15\n",
      "96/96 [==============================] - 0s 194us/step - loss: 0.6831 - acc: 0.5729\n",
      "Epoch 11/15\n",
      "96/96 [==============================] - 0s 198us/step - loss: 0.5996 - acc: 0.6562\n",
      "Epoch 12/15\n",
      "96/96 [==============================] - 0s 193us/step - loss: 0.5569 - acc: 0.7188\n",
      "Epoch 13/15\n",
      "96/96 [==============================] - 0s 196us/step - loss: 0.5672 - acc: 0.7292\n",
      "Epoch 14/15\n",
      "96/96 [==============================] - 0s 203us/step - loss: 0.5245 - acc: 0.7292\n",
      "Epoch 15/15\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.5878 - acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying Accuracy on Test Set\n",
    "\n",
    "test_pred = model.predict_classes(test_set)\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parallel CNNs\n",
    "\n",
    "It has been discovered, that CNNs for music work best, when they have one filter that is detecting frequencies in the vertical axis, and nother filter that is focused on the time axis, i.e. detecting rhythm. Consequently, this is realized in a parallel CNN, where 2 layers are not stacked after each other, but first run independently in parallel with their output being merged later.\n",
    "\n",
    "To create parallel CNNs we need a \"graph-based\" model. In Keras 1.x this is realized via the functional API of the Model() class.\n",
    "We use it to create two CNN layers that run in parallel to each other and are merged subsequently.\n",
    "In the functional API, you pass the name of the previous layer in (brackets) after defining the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input only specifies the input shape\n",
    "input = Input(input_shape)\n",
    "\n",
    "# CNN layers\n",
    "# specify desired number of filters\n",
    "n_filters = 16 \n",
    "# The functional API allows to specify the predecessor in (brackets) after the new Layer function call\n",
    "conv_layer1 = Convolution2D(16, (10, 2))(input)  # a vertical filter\n",
    "conv_layer2 = Convolution2D(25, (2, 10))(input)  # a horizontal filter\n",
    "\n",
    "# possibly add Activation('relu') here\n",
    "\n",
    "# Pooling layers\n",
    "maxpool1 = MaxPooling2D(pool_size=(1,2))(conv_layer1) # horizontal pooling\n",
    "maxpool2 = MaxPooling2D(pool_size=(2,1))(conv_layer2) # vertical pooling\n",
    "\n",
    "# we have to flatten the Pooling output in order to be concatenated\n",
    "flat1 = Flatten()(maxpool1)\n",
    "flat2 = Flatten()(maxpool2)\n",
    "\n",
    "# Merge the 2\n",
    "merged = concatenate([flat1, flat2])\n",
    "\n",
    "full = Dense(256, activation='relu')(merged)\n",
    "output_layer = Dense(1, activation='sigmoid')(full)\n",
    "\n",
    "# finally create the model\n",
    "model = Model(inputs=input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 80, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 31, 79, 16)   336         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 39, 71, 25)   525         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 31, 39, 16)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 19, 71, 25)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 19344)        0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 33725)        0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 53069)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          13585920    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            257         dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,587,038\n",
      "Trainable params: 13,587,038\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "96/96 [==============================] - 0s 5ms/step - loss: 0.5221 - acc: 0.7188\n",
      "Epoch 2/15\n",
      "96/96 [==============================] - 0s 284us/step - loss: 0.2603 - acc: 0.9167\n",
      "Epoch 3/15\n",
      "96/96 [==============================] - 0s 278us/step - loss: 0.1940 - acc: 0.9479\n",
      "Epoch 4/15\n",
      "96/96 [==============================] - 0s 276us/step - loss: 0.1510 - acc: 0.9688\n",
      "Epoch 5/15\n",
      "96/96 [==============================] - 0s 287us/step - loss: 0.1230 - acc: 0.9792\n",
      "Epoch 6/15\n",
      "96/96 [==============================] - 0s 264us/step - loss: 0.1052 - acc: 0.9896\n",
      "Epoch 7/15\n",
      "96/96 [==============================] - 0s 267us/step - loss: 0.0909 - acc: 0.9896\n",
      "Epoch 8/15\n",
      "96/96 [==============================] - 0s 266us/step - loss: 0.0788 - acc: 1.0000\n",
      "Epoch 9/15\n",
      "96/96 [==============================] - 0s 278us/step - loss: 0.0700 - acc: 1.0000\n",
      "Epoch 10/15\n",
      "96/96 [==============================] - 0s 277us/step - loss: 0.0627 - acc: 1.0000\n",
      "Epoch 11/15\n",
      "96/96 [==============================] - 0s 283us/step - loss: 0.0557 - acc: 1.0000\n",
      "Epoch 12/15\n",
      "96/96 [==============================] - 0s 294us/step - loss: 0.0509 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "96/96 [==============================] - 0s 266us/step - loss: 0.0457 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "96/96 [==============================] - 0s 268us/step - loss: 0.0420 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "96/96 [==============================] - 0s 259us/step - loss: 0.0388 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set\n",
    "\n",
    "Note: The functional API, i.e. Model() does not have a convenience method `.predict_classes()`. We therefore do 'raw' predictions with `predict()`, which returns values between 0 and 1, and then round to the nearest value (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.85507832e-03, 9.94681537e-01, 6.49887472e-02, 1.46704987e-02,\n",
       "       1.18694408e-02, 5.33168577e-02, 9.35246348e-01, 2.63482660e-01,\n",
       "       7.10115358e-02, 6.32808864e-01, 3.26535362e-03, 8.87194633e-01,\n",
       "       6.38452638e-03, 5.17982185e-01, 8.89571756e-03, 9.56415534e-01,\n",
       "       9.60347475e-04, 9.35900360e-02, 3.70097905e-02, 9.27556634e-01,\n",
       "       4.83055979e-01, 8.66333246e-02, 9.91402805e-01, 7.15953037e-02,\n",
       "       9.09844518e-01, 9.97280295e-05, 3.09065566e-03, 9.93965924e-01,\n",
       "       2.06456222e-02, 9.93725121e-01, 1.24999015e-02, 7.67252624e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(test_set)\n",
    "test_pred[0:35,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = np.round(test_pred)\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
