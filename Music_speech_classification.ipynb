{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music vs. Speech Classification with Deep Learning\n",
    "\n",
    "This tutorial shows how different Convolutional Neural Network architectures are used for the task of discriminating a piece of audio whether it is music or speech (binary classification).\n",
    "\n",
    "The data set used is the [Music Speech](http://marsyasweb.appspot.com/download/data_sets/) data set compiled by George Tzanetakis. It consists of 128 tracks, each 30 seconds long. Each class (music/speech) has 64 examples. The tracks are all 22050Hz Mono 16-bit audio files in .wav format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "This tutorial contains:\n",
    "* Loading and preprocessing of audio files\n",
    "* Loading class files from CSV and using Label Encoder\n",
    "* Audio Preprocessing: Generating log Mel spectrograms\n",
    "* Standardization of Data\n",
    "* Convolutional Neural Networks: single, stacked, parallel\n",
    "* ReLU Activation\n",
    "* Dropout\n",
    "* Train/Test set split\n",
    "\n",
    "You can execute the following code blocks by pressing SHIFT+Enter consecutively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "**On the lab machine this data is already prepared, no need to download.**\n",
    "\n",
    "The data set can be downloaded from [here](http://opihi.cs.uvic.ca/sound/music_speech.tar.gz). Please unpack it (it is .tar.gz compressed)\n",
    "\n",
    "Set the path to the unpacked folder in the next box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you use the iDSDL Lab machine\n",
    "DATA_PATH = '/home/ffg/idsdlmteacher1/Code/DeepLearningTutorial_2019/data/Music_speech'\n",
    "# if you downloaded the dataset into the tutorial folder (otherwise adjust the DATA_PATH please)\n",
    "# DATA_PATH = './Music_speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use Multi-GPU set this before the import of keras or tensorflow:\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import librosa  # for audio loading and preprocessing\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # for reading CSV files and easier metadata preparation\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Deep Learning\n",
    "from keras.backend import floatx  # abstract default float type for Keras (e.g. float32)\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, merge\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Metadata\n",
    "\n",
    "The tab-separated file contains pairs of filename TAB class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>speech/stupid.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/teachers2.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/danie1.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/oneday.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/jvoice.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/relation.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/geography.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/pulp2.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/greek1.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/conversion.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            1\n",
       "0                            \n",
       "speech/stupid.wav      speech\n",
       "speech/teachers2.wav   speech\n",
       "speech/danie1.wav      speech\n",
       "speech/oneday.wav      speech\n",
       "speech/jvoice.wav      speech\n",
       "speech/relation.wav    speech\n",
       "speech/geography.wav   speech\n",
       "speech/pulp2.wav       speech\n",
       "speech/greek1.wav      speech\n",
       "speech/conversion.wav  speech"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = os.path.join(DATA_PATH,'filelist_wclasses.txt')\n",
    "metadata = pd.read_csv(csv_file, index_col=0, sep='\\t', header=None)\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two lists: one with filenames and one with associated classes\n",
    "filelist = metadata.index.tolist()\n",
    "classes = metadata[1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Labels to Numbers\n",
    "\n",
    "String labels need to be encoded as numbers. We use the LabelEncoder from the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speech', 'speech', 'speech', 'speech', 'speech']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 classes: music, speech\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(classes)\n",
    "print(len(labelencoder.classes_), \"classes:\", \", \".join(list(labelencoder.classes_)))\n",
    "\n",
    "classes_num = labelencoder.transform(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first few classes (speech)\n",
    "classes_num[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first few classes (music)\n",
    "classes_num[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In order to correctly re-transform any predicted numbers into strings, we keep the labelencoder for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Audio Files\n",
    "\n",
    "The audio is stored as .WAV files. We can load them directly. Otherwise tools like ffmpeg can help to decode. librosa.load supports the use of ffmpeg.\n",
    "\n",
    "Audio files are usually preprocessed the following way before using them as input to a CNN:\n",
    "* Conversion to Mono\n",
    "* Cut a segment (or multiple, to create more instances)\n",
    "* Compute a Spectrograme using FFT\n",
    "* Convert to Mel scale (a psycho-acoustically motivated perceptual scale, also useful for dimension reduction)\n",
    "* Convert to Log (or Decibel) scale\n",
    "\n",
    "The FFT parameters as well as the desired number of output frames and Mel bands can be set below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS\n",
    "AUDIO_PATH = DATA_PATH\n",
    "\n",
    "# some FFT parameters\n",
    "fft_window_size = 512\n",
    "fft_overlap = 0.5  # 50% overlap between successive frames\n",
    "\n",
    "# desired spectrogram parameters as input for the CNN\n",
    "frames = 80        # x axis\n",
    "n_mel_bands = 40   # y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................\n",
      "Read 128 audio files\n"
     ]
    }
   ],
   "source": [
    "list_spectrograms = [] # spectrograms are put into a list first\n",
    "\n",
    "# FFT parameters\n",
    "frames = frames - 2  # for librosa, which otherwise creates 2 frames more than needed\n",
    "hop_size = int(fft_window_size*(1-fft_overlap))\n",
    "segment_size = fft_window_size + (frames-1) * hop_size  # segment size for desired # of frames\n",
    "\n",
    "for filename in filelist:\n",
    "    print(\".\", end='')\n",
    "    \n",
    "    # 1) load audio as waveform using librosa\n",
    "    audiofile = os.path.join(AUDIO_PATH, filename)\n",
    "    # auto-convert to mono. to preserve the native sampling rate of the file use sr=None.\n",
    "    wave_data, samplerate = librosa.load(audiofile, mono=True, sr=None)\n",
    "    \n",
    "    # 2) use only a segment; choose start position:\n",
    "    #pos = 0  # beginning\n",
    "    pos = int(wave_data.shape[0]/2 - segment_size/2)  # center minus half segment size\n",
    "    wav_segment = wave_data[pos:pos+segment_size]\n",
    "\n",
    "    # 3) FFT + Mel-transform in a single step  \n",
    "    spectrogram = librosa.feature.melspectrogram(y=wav_segment, sr=samplerate, \n",
    "                                                 n_fft=fft_window_size, hop_length=hop_size,\n",
    "                                                 n_mels=n_mel_bands, fmax=None)\n",
    "    # Note: one may want to limit the maximum frequency in the output spectrogram using the fmax parameter\n",
    "    \n",
    "    # 4) Log 10 transform\n",
    "    spectrogram = np.log10(spectrogram)\n",
    "\n",
    "    list_spectrograms.append(spectrogram)\n",
    "        \n",
    "print(\"\\nRead\", len(filelist), \"audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661500,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20224,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_segment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20224"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 80)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An audio segment is 0.92 seconds long\n"
     ]
    }
   ],
   "source": [
    "# duration of the segment in seconds\n",
    "wav_segment.shape[0] / samplerate\n",
    "print(\"An audio segment is\", round(float(segment_size) / samplerate, 2), \"seconds long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For simplicity of this tutorial, here we load only 1 single segment of ~ 1 second length from each audio file.\n",
    "In a real setting, one would create training instances of as many audio segments as possible to be fed to a Neural Network.\n",
    "\n",
    "### Show Waveform and Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'music/echoes.wav'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XecE3X+P/DXm4WlL3XpZQFB6QIrINJUUAEFz4pnwYrlPPUsd1jPn94pip7t/KqcHQt2RUEUEBSRtvTeF1hYlt7r7n5+f2SyO0lmkpnMZJJNXs/HgwfJZDKfz04m73zmU0UpBSIiSi3l4p0BIiLyHoM/EVEKYvAnIkpBDP5ERCmIwZ+IKAUx+BMRpSAGfyKiFMTgT0SUghj8iYhSUPl4Z8BM3bp1VVZWVryzQURUpixYsGC3Uioz0n4JG/yzsrKQk5MT72wQEZUpIrLZyn6s9iEiSkEM/kREKYjBn4goBTH4ExGlIAZ/IqIUxOBPRJSCGPyJiFIQgz8RUQLYc/gEflyW71l6DP5ERAngtg9zcOfHC7H3yElP0mPwJyJKAFv2HgMAFBYVe5Iegz8RUSIRb5Jh8CciSkEM/kRECUF5mhqDPxFRnCmlsPuwr6FXPKr3YfAnIoqBmet2Yfm2A5b2/W7x9hjnJlTCzudPRFSWXf/OPABA7ughEffdcfB4rLMTgiV/IqI401f0CHv7EBFRrDD4ExGlIFeCv4hcJCJrRGS9iIwKs98VIqJEJNuNdImIkoG+k+dj3yz3JE3HwV9E0gC8DmAQgHYArhGRdgb7VQdwD4C5TtMkIkpWk1fs8CQdN0r+3QGsV0ptVEqdBDAewDCD/Z4G8DwA75u1iYhiQCmFN3/dgG37j8U7K7a5EfwbA9iqe56nbSshIl0ANFVK/eBCekRECSFv3zGM/nE1bnl/fryzYpsbwd+oY1JJFZaIlAPwEoAHIh5IZKSI5IhIzq5du1zIGhFR7BQrX6g7erIozjmxz43gnwegqe55EwD64WrVAXQAMENEcgH0BDDBqNFXKTVWKZWtlMrOzMx0IWtERIlPeTutDwB3gv98AK1FpIWIpAMYDmCC/0Wl1AGlVF2lVJZSKgvAHABDlVI5LqRNRBR3yuNJ2dzgOPgrpQoB3A3gJwCrAHyulFohIk+JyFCnxyciSlSxmoTNiwVdXJnbRyk1CcCkoG1PmOzb3400iYjiLdoSf3GxggggJnM5XP/OPHw6sqeTrEXEEb5ERA7ZvQNo+cgkPPHdipLnE5cFzuo5e+MeV/IVDoM/EZFD0dwBjJuzueTx8m0H3cyOJQz+RBTWXz5eiOvf4cB8I14tvBILnM+fiMKauCw/3llIaicLY9+4a4QlfyKiOCqORyd/MPgTEXlq1vrdAc/jFPsZ/ImIvHTt24nRfsLgT0QUR14t2xiMwZ+I4u7VaetwwUu/xjsbhuZs3INjESZu27r3GF6fvj7sPkXFCmN/2xCyndU+RJSy/jNlLdYWHI53NkJs3XsUw8fOwaivl0bcd8xPa7B+5yHT1ycs2YZnJq12M3uOMPgTEZk4ePwUAGDNDvOgrnfBS7+ZvnbkRGJN+8zgT0QUgdkcPMGKw1ThJNq8nwz+RClq+bYD+H/fr4CKV6Vzqkmw88zgTxQnN743D7d+EL9lLa5+azbem5WLI2VwFSqvuBmvEyv0c3oHipNTRcUoX04s304noxlr4rtUKYO+dWZXaVm+fFnyJ8/tOnQCrR/9Ee//kRvvrBBZ4kap3ewuIl6rgDH4k+fy9h0FAHy7eHuEPUlvy56j2LT7iOvHXbnd++mEwzlZWIyHv16GgoPH452Vkmt1VX7052jzniOYYzI/vxcrdplh8Kf4SbAGsETXd8x0nPvCDNePO/a3jciNwY9KtH5ZvROfztuCx79dHu+s4PegeXii0W/MDAwfOwcHjp0Kee2t3zY6Pn60GPzJc/56/lQI/dv2H8Pv65wHkFiauqoA/WPwoxK95Lwy/jNlbci2CXG8+2XwJ4qhAS/+iuu4EEpMKKXw4ezciFMvJLI1BYew5/DJuKTN4E8UQ8dOlZ3AtO9IfIKQ3iSThWPy9h3F5OWBr01ZWYAnvluB0T+usp2OUgofz92M4wnw+UxYEp/SP4M/ea4M947zRHGxQnG4oaIx0uXpKRH3efCLJZiysiBmebjr44W4d/xiAIGLnAz97yzc8dHCgH2PaiX+/QZ16ZFMXr4Dj36zHC/+vMZBbt0x5qf45IHBnyjBDPjPr2j3z8kB24ps/BicKirGkROFYfcJt3Rg3r6jyBo1EXMNeqh8uSAPt30Y24FpJ7S8TV21s2TbXpfvSg5p52fvkfA/HMncJ4HBn+Immb9YTmzcfQTHTwUG53d+t94r5Lq356L9P38Ku8+GXeYzaM7ZuBcA8FnO1rDHeGnKWvQbM91yvuLhh6Xbsa7ANynbweOnsHzbAQCld59O+9jH8i4o1jjClzxTXKxQrFSZHhVplVLKVmk9kq17j1ned+6mvRH3CfcZWJ3r55Vp66xmydQbMzbg2MnwdymRhMvu3Z8sAgDkjh6Cm9+bj5zN+7Dp2cGujSx/6oeVrhwnHljyJ8+MeG8eTnv0x5Ln8RrZ6IVXpq0L+FvjZfy8LfjRoBFVEqTl5bnJq/HqL+EXQTFjN37nbN4Xsm3/0VN4/NvlOFHoXsPvgWOnMH3Nzsg7xhlL/uSZmQne3z1aU1YWYGnefjxwwekl28bPM68y2bDrMH5ascOLrGHU18sA+Eq+em7+8K7KP4i2DTNcO16sKVVa7fPLal+Q7tikBq7KbhrxvXn7juL4qSKcVq+66T53frQAf2wwHtGbSFjyd+BEYRFemrI2IbqLJbqc3MhVEXYcOVGYMOf9tg9z8FpQ6fVwmAbXq9+ajecnl/bweNWk+mT34ROmx9iy5yhOf+xHbNx1GKvyD2KqhbrnPzbsRva/pkZsDLZr0CszXT2eVf7qHle6Sup+C7NGTcTLU0MHZAFA7+emY8B/zBdsAZCQK5IZYfB34L1ZuXhl2jq88/smS/vPWr8buw6Zf6GT2VjdMPah/50FwHqD79GThdi850hA98f2//wJA2O45uvxU0VRd7dcv/OQYfDfe+Qk+o+Zjt1Bg3r0Iz/1VTTZ/5qKlg8b97r5bvE2nCgsxlcL8zDolZm41UIPnDE/rcHuwyew2uKqVNEqLCouWQErESmEVhn9/auluPi10h+xl6dG354R7kc7kTD4O+AveZ6wWAK99u25uOqt2bHMUsJyUslw/Tvz0G/MDFz82u8B2+00gtp1xuOT8fevIq/basSs5Dd1VQFy9xw1fO3g8VNYv/MQ7vw4sC97sQL+N9O8p0+0dfdWfniLihXyD9g/x49+sxydnvzZlUnL7h2/CFmjJjo+jp4y6XSwfNvBkC6wydsq5VLwF5GLRGSNiKwXkVEGr98vIitFZKmITBOR5m6kGysFB4/jgpd+xfb9vgt//Lwtro1+jMWsjHacKipG1qiJGD9vS9j9Dh0/hR0HrM+qGMvVoBZoDXUrHcysGI0vF+RF9b5ownGnJ3/GiHfnG76m7+/u50VQ+m7xdpz97C+Gry3aEtp46vfVQt95cyOP34WZ+0bfbmHnh2bL3qNYmnfA8LU2j8W/kd4rjoO/iKQBeB3AIADtAFwjIu2CdlsEIFsp1QnAlwCed5puLH02fyvWFhzGp/O2YF3BIYz6ehnuGb8o3tlyxeHjvuqI0ZNXh93vwpd+Q89np1k65tK8/Wjx8KSEm8Ds+yXbbQ8OGvzKTNM6+GBKBZaMs0ZNRPd/T8WybcaBJZJt+62Xsp3+1m7b5+yuKXePeSHGPzK3XIz79G7fX1o4mWyjAf28F3/Fe7NyY5CjssWNkn93AOuVUhuVUicBjAcwTL+DUmq6Usp/vzsHQBMX0vWEf7Sh0eRLdm65zUrGRcXu9gd3y3at1G80DW2wudqgoHDd25Zs3R+yzX9KYjGdcMHB4/jrp4twx7gFtt63Mv+g4eyLRj6dtzWkZLzz0An834wNttKMxtjffGms3mH9bkh/CYZrI4jmapyt693iv5wFwMWvzcRN782L4ojGPp+/FbsPn8DB46fwva6h95RByf9pF/rgJ/NARDeCf2MA+n5tedo2M7cAMLy3EpGRIpIjIjm7dsVviTujD3xl/kH0fi7wi/7FgvAjIAFgxfYD+HnFDtOLqMcz03DmUz9Hk01PXPHGH5i+eicWGwRvP38Bz+xvLDh4HDtNGrq/XbQN/V+Ygd/Wuvt5++tut9uoszZqWA27/yZ7+7tVDl6x/UDJEox5QSX4PzZ4c/f1t8+WBDy/5n9zMG52bkj9/PJtBzHdxeUq//7VUpz176l46IslAQ3Xuw+FFs6sdsQIZ21BbBvH48mN4G90TRuGARG5DkA2gDFGryulxiqlspVS2ZmZmS5kzV3BX7Tg50aGvPo7Ro5bYFqa2n34BA5pVTFFxQoLNlvrEvnu75s86WK3budh3PT+fFz6+qyI+5r1He/xjHn1kb/uNdovmb8N4wWHk2PNz92Lq8fOsfWecPXRsTTk1d9NX1uxzfxOoLQWJjbF2ce/W2GSnruUAn5aEdi19d+T7M/sacUCg4Fhfol4x26HG8E/D4B+dEQTACHfChEZAOBRAEOVUgndF8ruRavgqx5ZsT26ul4A+G3tLjw5YQUuf2N22Asu/8Ax/O+3jXjqh5WOlpYDgK17j7rSUOsfKr9p9xFb3SPd+Or4q+Xem1Vayvt93W5b9ecAcOWb1nth7Tx4PKqG+xd/tladZMfmoN5D1uaaMb/At+417o3klFH3x+02PyMnNodpozBjVJWkV9Ybh90I/vMBtBaRFiKSDmA4gAn6HUSkC4C34Av8CT/u2W48VAro9ey0sCUyfZA1aoS84d15GDdnMwCEXbt05IcLLJdyCouK8fi3yw276+0/egp9np+Ot2f6gubRk4XIGjURf/lkoeWudY99uwzP6RqOZ6zZFTLYKR6ue2cuhtssxZvp+M+fQnqSdH9mWlTLKe6IwZq0wesFzNMNpgvua2+l/3mf592bqO2Erttk9r+mhvywhKtKdFukQG4k0ijslC/5K6UKAdwN4CcAqwB8rpRaISJPichQbbcxAKoB+EJEFovIBJPDJRyrPwT+OlizhZr1h3ns22VR5+dQ0Bf6RGERxv62wbCr2x8b9mDcnM34x1el6QX/OfO1YOFv0J641HgxDb9TRcVYo9W1fjRnC96YsSFgdGmOxWqrSE4VFePZSavwh8U1VI9EsZrT8m0HkDVqIsbNzjXd59CJwpJquXgqjuIO7fp3ShtalVIl4yK+iDBbp1sN8CeDrkn93diq/IMBjcSJyD8pXLJyZW4fpdQkAJOCtj2hezzAjXQSQZ/nf8FP9/VFlXTjUzd87JyQeVSAwB+RE6ecD37xe2PGBrw8dR2qpJfHdT0Dh0/4k/TfdUxdWYB9RwPvOvwjUa12T/z3xFV4/49czPz7uSXbZgf94E1ZWYB9R07iqrPCz5Wivxv618RVqFQhDQ0yKiGjcgXMXLcLb/220dYC13bnmPEPGot2YjEvfTw3/LgMI/oeVv67SgAYPz988Hert1K42tNYtVdljZqIe847Dffr5lmKBTcngouXlJrYbeeh43jr1414ZHBbpJWLXLFvtMfWvcewKv8QujWvZfq+Wz/IwUUdGuCKbqU9Wm94N3Ad1wc+X4Jt+53Xr/r77VtZx9Soe59/Aqq7gkaWGpm8PB/fLNoGwFdtZORUUXHJYh+Rgv/qHYfQq1XdkuePfbu85PGfuoTrMGZs56ETKCf2G44jTbmhbwOy25bgpYe+WGL6mtnyiHbY6VYaT6/+sj4k+Eeaj8euP73+h6vHi4ekm97hyIlCvDZtHZbmhdYnPvL1Mrzz+ybMXGet61m0NXpTVxXgwaAvon+BDMC3/NxXC/MCtukVFiu8P2uTYT1lPGsZ7/hoYUm/f7PGZv3fFG61qEjMfpp3HjKvNxcAF77s7pc82DmjjUe8JoIvwoxIdmMK56Vb7XVoCO7B9eyPq7H/6ElLY0cSkb4tzOvR5rGQdCX/E4XFeHHKWlSvVB6dmtQE4OuFcuFLv6FhzUoAQgPoTyt2oGGNSiX76xl3X3QWgoOrSYJ9NHsz5uXuxcmiYozs2yrgteDeHWY5WbH9QNiGY6eszHvz4pQ16J5VO7oETGLVR7M3G79gdhiT49iZfCtR5r6PN7tzHelH4AK+aqgzn4q8TjB5I+mCf5r2bX/y+5UY0K4+mtSqgm8W5uFkUXFI4PS7XRsFGlxXH+4rP0zX7z3fZA6caLt++nts+Bsas0ZNxI29stC0dhVL77/qzdkBvT7i5ecVBXjr1/B19nbnlQ+un++gW67QKNAbtZP+sroAN79vfR3auZv2YGC7+nghARb7jlYqrJ5G9iRdtY9epH7VefuOYuX20Ns3KwFJ35jmn8gqWLiun1bov6/v/5FrOFzd6DttFPiDewnpxaoe2+tJ7PS9W/yCB+KtLThkK/ADwMhxC/Dlgjy8Pj320zYkE69/cDaGWZeYQiVdyd8K/zXZ+7nAPs07Dx7HuDmbYz4hlZuOWpxOetZ686qmeNZjm02wFauqlpvfN545M5KHvoxueme/cIu7JCuvq8vCtXlQqOQr+euut3CjV42+jPd/vgSv/bIei3SleqNDXP6GR3PyW/gR+kTrAliGfq8scfPvcXveoGjoq6dShddrNL/hwYR6ySSpS/7fLt6Ol4d3MZzu1ejL6B8tGcu56W2xkY9EybJbop1L38iEJdsxe+MebN5zJOnOk1XxKBzEcxBXwnyHE1hSB38AWJZ3IGRlJbPLYl2CzeC36/BJ7LTYY2dJ3n68+esG3NGvVeSdU4z+h6RxzcpxzElqiWZgmlvMulFTqaSr9gku4Rg1dJoVgg5qvWtmJsiiJJ/O24LuYWbE1PthaT5G/2i8QEtRsULevthM2FXWJPIgLXLP5xGmsKAUKPk7vflLhpvHPzbsKRnJS6kp1cYq+Eeik7mkK/kHc9I3e/z8rdgdYeg/UVkQzcRwlNySLvgHl28WbQmd5sHKPDiAb66Y28bZ6xNOlIjMFiyn1JV0wd+KOy1MYuZX1gpM0cxbTskvFccZUHgpGfyTmd0Fy4koNSVd8JdkG+1k07TVCb9QGhElgKQL/kREFBmDPxFRCkq64J/alT5ERNYkXfAnIqLIGPyJiFJQ0gX/FO/sQ0RkSdIFfyIiiozBn4goBSVd8E+12QuJiKKRdMGfiIgiY/AnIkpBSRf82duHiCiypAv+REQUmSvBX0QuEpE1IrJeREYZvF5RRD7TXp8rIllupEtERNFxHPxFJA3A6wAGAWgH4BoRaRe02y0A9imlTgPwEoDnnKZLRETRc6Pk3x3AeqXURqXUSQDjAQwL2mcYgA+0x18COF9SfeJ9IqI4ciP4NwawVfc8T9tmuI9SqhDAAQB1XEibiIii4EbwNyrBB698a2UfiMhIEckRkZxdu3ZFlxneTxARReRG8M8D0FT3vAmA7Wb7iEh5ADUA7A0+kFJqrFIqWymVnZmZ6ULWiIjIiBvBfz6A1iLSQkTSAQwHMCFonwkARmiPrwDwi1IqpORPRETeKO/0AEqpQhG5G8BPANIAvKuUWiEiTwHIUUpNAPAOgHEish6+Ev9wp+ma4dw+RESROQ7+AKCUmgRgUtC2J3SPjwO40o20iIjIOY7wJSJKQUkX/Nnbh4gosqQL/kREFBmDPxFRCkq64M9aHyKiyJIu+HPwABFRZEkX/ImIKDIGfyKiFMTgT0SUgpIu+JcvxyZfIqJIki74c40YIqLIki74ExGVdXWqpsc8DQZ/IqIE06lJjZinweBPRJRgvKi+Tsng36FxRryzQERkyouWy5QM/peeGby+fGp55k8do3rfv//UweWcEFG8pGTwB4CHB50Rsu3GXlneZ8RjQzo2xJ97NEO1iqXr+Azu2MDSe6/t0TxW2SIij6Vs8L+ldws8MLBNyfPy5QRPDm0fxxzF3hkNquP1a7sCAPRLKD90YegPIRElt5QN/uXTyuGv57cuee5vX1nyxAWon1ER556eGaeceaOYM+DZdmW3JvHOApFrUjb4B/MXhGtUqYC5jwzA1Wc1jXmaaXEcjVyszKP/a9d08TAnZccTl7SLdxYoRXgxVjXlg3/fNr4S/i19WgS94m1gvvvc0zxNr15GxZLHTWpVDnjtks6NULda7AeZlDXVK1WIdxYoZbCrp2O9WtUJ+3pdbSRd63rVvciO5ybe09tw+/iRZ5c8rpBWDpee2Sjg9dv6tIxpvogovpI++H9yW8+wr48adAYu6dwIQzo2DNje0YMRdk5USLNWMmjfqAaqpKcBAK7tWdpbp3HNymZvAQCUi3DfObBdfUvpJ4v2jXxjQ355oF/UXWWJrIt9o1zSB38jFcuX/tn1MirhtWu6oLIWIP0a16yM3NFDYjog7JzT6pY8rlnFXpXCfQPaRN5Js/zJC7HxmcG4vqd7XTW7Nqvl2rGc+uDm7rbfU058vZ/C0X8mX93ZCwDQMrMa/tyjGZb88wLbaRIlkpQL/vec3xpXn9Us3tkAALx1XbeS+vb+p9cLeO3iTg2N3lKiyEZ3nXLlBOUMGpcb16xs2sd/QFDJvkblwB+nRBolrcI0Xpu5pHMj1KoSvl2jeqXSsRCVKgQWDoLPR7zc2ju4rYrImpQL/vcPbIP08onxZ1dOT8PzV3RC+0YZaFa7SsBrd58XvgG40ELwv71v+Hr7WaPOw/9d283wtRZ1qyJ39JCSu4V7dd1iAaBP6+i7wmZWrxh5JxuiiP0AIveo+FMX97p2xqpj1z8MBitSMoh9g2/5yLukNonxh9CrVV1MvKeP7fcVWwj+nZvWjCZLAfwB0q3gVbtqekC1WzSeu7wj/vHVspLn4bqtOnFBu/qoXCENq/IPOj5WOZGY5JOrVyQndvVMQNf19LbK6PT6xvXSRSaB5I9R55U8tnP9mM0iWCGtXNjX3dS2obWqpHoZlQKe1w96bkXXZrXCfsG6NquJDo1r4M7+rfCqC+MeIjWgR6t8mvlX+B7d3eOQCNWIyYZdlSNj8E8gdqqjjOr8ZzzYP6CPwLln1AvZx677BrTGjb2ySga9fXBzd/zwV1/30a/u7IW/2Wh4joW1/xpU0ljfrHYV1LNYpXTD2c1ROageP5ac3u1Eo0mt0qrEm89h20BZEqOb2QAM/i6oXtGd2rP5jw4oedwww9cQPLy78Ujj4J47b9+Qjay6VQO2BTdSRqN6pQp4cmj7kmP1a5OJDo193WC7Na+Fewe0Dvd2W/Rl4zFXdDLc5+lLA2cWTSsnJV+U8mmCuY+cby0tEYy+3DgNwP2Odr1b1428UwyV1aWtY3HDZHUiw3hK+GofEaktIlNEZJ32f0j/PxE5U0Rmi8gKEVkqIlc7STMRLY7Q7e+SzqUDqG4J0ztD34OkRpUKyB09BDed0wJf3HF2yFxDTWtXQet61UyP1aiGvaqQodogL/1kd17TB9wrsyP/6J3RoLoW/H3vFBhXT/nfkzt6SMD2utVC7xLMfnSMvDMi2/K++mvg8YtjN02E2ZxUZWFt6xZBhRcAqJaems2SZWE+/1EApimlWgOYpj0PdhTADUqp9gAuAvCyiDhvifSIlS6NkT6oobovfqR9P7/9bMz8+7kB287Kqo33bgrty67viugXTbdHADj39HrIHT0kYLK7WDg/iqqo4KDtP4fBdf9mAe7pSzuEHMNMy8yqAWmEc37b+vj+buMR1MEGd2xY8nld0a0J/twjNm1Hz15W+uOlHN6/TL2/r9Ps2FLbYN3a2/t5N9I8ke6OEr7kD2AYgA+0xx8AuDR4B6XUWqXUOu3xdgA7AZSZKTOfHNreceOR/nOM9KF2b1EbTYO6fQbzDzh647rSbpoZQf3OE7Wk98xl5qNjo82xP8TF4y8ONxL8qWHmU4R7sXaEvhxg59z4R4Q3qGE8CrydxYZ5N8SiW3ZVk7uJVJvo1uk9VX2lVD4AKKXyRSRssU5EugNIB7DBYbqeqVg+DTmPDcTSvP2oWrE83pu1yfYx9BewG70+ujX31a7Vz6iE1U9fhOmrd6J7i9oASn8EErVes0KY3il2B07573L8Qc7N3zs3AsHws5rhie9WuH7w2lXTsffISWcHCSPSaaxVNbYD3D4b2ROVKqShdtV0TF6+w/XjP35JO3yxIC9kuxeNrIkk4s+qiEwVkeUG/4bZSUhEGgIYB+AmpVSxyT4jRSRHRHJ27dpl5/Ax16lJTbTKNK9jN7Lw8YFY9PhA9NZN4+B28bRShTQM0s1LlFGpAhY/MRCjBrV1N6Ew/L1/nLLapdLfJdTfA8lfveFkTEbpLb/9Y0y8p7dhFVx6+XLo2LgGng2+23F4DRilZcRpLAuuQvQXMG7s5V7PoQWPDQjZllZO0LlpzYh3wFZlNw9sisyoVAGnGbSX3dm/lSvplRURryKlVOinoxGRAhFpqJX6G8JXpWO0XwaAiQAeU0rNCZPWWABjASA7O7vM/A6blTiN6jBjPWgMAGpGmLbAbf7ePwCQnlYOJ4sMf9sjCh7526lJDVxnMB9R/YxKAXX4xVpyVkr+n99+NqpWDO0FtfDxgThZVIyte4/ayzR8k+fVqZqOQ8cLQ1773qUfRqv0XV2PnAjNjxX+KkP9F7BF3aol3VXdrIqpU60ibu/bEgePn8LagsMGeXGehlHDfjD/9fTGjDJTKeGY009xAoAR2uMRAL4L3kFE0gF8A+BDpdQXDtOzZPJ99kfMWmU1ePdsWdtwe982dfHejWfh3Rut9xTx69Y8cSZTC/bAwDa4sVcW/vtn89L713f1snXMCXf3xlUmvX70GmvzI910ThYAX5vI+zedZbhv9xa10b5RaD19zSrpqFfd/mAxP1ttLBaLNWbjEMyqJ8oHzeE0Ycn2ksdWs1e3WsWSK1yfzpd3nG24P+B8DMHDg9sGNFTrdWzsfHZdO9OJPDK4dLqMSBP/lXVOg/9oAANFZB2AgdpziEi2iLyt7XMVgL4AbhSRxdq/Mx2mG9YZDTLw8a09MP3B/jFLQ3+RGOnXJrD5w38BtsqshnPPqIfzzrA/JfInt/VI2Nkk/3qftBs+AAAR3UlEQVR+azw5tH3YBU9iNRNojcq+brH+Cfu6Na+F/qfXw+MXt4ti+uXoipqxuJ87dqrI0ftHnJ1l+z0zHiodKCjazKdt6ldDnWoV8eiQtujarCbOygr8HCun+8JI8PxUdvnns2qtG9XeuFb4qcfdNrJvadXPCA8a5c14UUPgKPgrpfYopc5XSrXW/t+rbc9RSt2qPf5IKVVBKXWm7t9iNzIfzjmn1Q3pN1zNhcFYl2vruF7QrrRBVV/q89eLdmkW2JvVjUbJiuXTEmY2STOJ1Mnolt4tYtalMoTNv9uLxkX9zKwNLI770H9HBMDk+/ri57/1A+ArVH191zmoYtJbJniN43DTdRiNePZ3N473Nf7YkLa4pnvTgM9oWNBiR8kgJUb4fn93b1zZrYkrDZNnNq2J3NFDQkbT+hndMvue+zbEao6XsiTVelX4uf3JR5pnqkblCtj07GCsfOrCqKq0rH5M/lJq8P51DNq8/Cbc3RsPW5iR1F+wapBRCTNcvJP/96UdShboCXZrn5Yh1VBedM31WkoE/45NamDMlZ1NA3YsBA+wuf8C38jZDK4Dm1AGtK0fsIpbpQq+r4SVRkI9O4E9msFXv//j3JBtVpbaFBHTkrrpe2ztXXq3p1TpGBQA+M9VnU3fc3qD6ri9X+TeNf68iMDV72+PlnUizqar/5wUfNeKV8rCIC8KclnXxgBCh6pf26M5ckcPSZi1BMjn7RHZeP3ariXP2zeqgecv74QXrjAPXEbcHlQXPGGevgdXVa1qxn8X6f/BqhWmpG2Hv33K6l1qyd0uVECnhOAR2NHw56FKur15qtLDjCexKvgONdm6gjISuezqs5ph4zOD0dBkdCQlvqvOaooaNpfVdLugpp8w76ELTw84/jsjsvHwoDNKVoEb0tFXH32PS1NzfHxbD4y5opMrbWRONahRCQ8POgPvB01vkl6+HO7X5qG6+9zTsOGZwSXtbS3rVsVHt/bAJ7f1CKnqDW6sNuqO7aeP/clYWRv/TzcJGS2ZmGoaZFTCjoPH450Nz1gpJUd7d3Du6fUCqgEa1awcUGXiv9zS09y57hrWqGw6sZ4T3/3lHAx7fZbt90WqHhLxDQzzr+tw74DWaJlZDS21QZmNa1bGsVNFeHpYh5C1umf+/VwUFhlXw51mc1BnWcOSP8VEszrujM5MZfqFfLzo+hc1/6CwCE0Z4UrZtunSipRurarpeOHKziGBH/BVoZnd5Z3dqg4yLI6mdhvr/InKiAHtnC+cEyxNK9IXK2UpGFj9gbiiWxNXq3RK6/y9pW9riJUWSVz6Z/AnV/lHpWYG9Zb5P12jajjBfcXLitpVrfcOUspawNL3orES/K0GwReu7OzavDkA0LeNb5LevmEWrGmV6W5POwVVUmJPxh50Xtzpsc6fXNW5aU08f3knXNSxASYuyy/ZPrijtTVkn7+iE563saBKorDyVbV7K1+uZI4dFTYYxHvoSLfmtSKulzDtgf5RzZsUzp+7N4MAGN7d23W1kwVL/i6qn2Gvb3iyuuqsplGXxkQkYdciCMdKlttodfjl08RSyc7fkFtsseRvh1dnWL+CmV/jms57winlW7z++rOzSqYJ71HS28e9qpqbtfmijFYZiyUvvgIs+bvkreu7oVOYhT3IGSezhSaK/92QjeXbDqB6pQpQOBZx/zv7t8IdHy1Ei7pVwwbr4AbPl67ujNb1EmNSsv9c1RlPDTVf1MZN1/ZohvPb1nO1m/WwMxtj2JmNtWdHXDtuJF7MBMDg75IL2yfm4inxdHV2U3yWs7Wk4dKJH+/rgwW5+1zIVWxY+QtrVK6Ac06zvpD7RR0allSnFBVbb9T8UxfjdpN5j5yPE4Xe/oBWSCvn2uCzSEQkacbXeNFbnMGfYuaZyzriyaHtDbvY2dUqs5rtxXS8FG1VVet61fD2iGz8vn43Vucfwrg5m42PHzZta2npR9w6LVguenwgujw9xdZ73CzMfnZ7T/cOloC8GCvE4E8xk1ZOXAn8ZUG0ga2cCJrXqYrmdXx1yqbBP8GaQaIpzfvvAK2uRBZOt+bG62UkizRW+xAR4P7cQfrD/XhvHwx6ZWbMu9k2rFEZT17SDhd2iL6K9NnLOqKlx42vfp09bNNzo6o0EgZ/IhfY/arW0iZq62Gy4psd1/fMwuc5eSX97a3QNxK3bZgRsaumkbHXd0Mjmz13bnS46tc1cezWWT6tHNo2zMCq/IMxT4u9fSipvXR1Z9SpWhE3vDuvZAnGsqpXUEPueybLSPrVz6iEXx7oFzLY6p+XtLO17CDgm7I8muDt1AXs5ODYmCs64aEvl4Zs96K7M4M/xU1289poWrtKXAKX21plVkPu6CHo+vQU7D1yEp0srD3b0qAB+yaHJWOrYhlbJt3Tx915fJKQiH+sgvEH4UWdPwd5UdwkWiMmuaNdowzLy0aWNdFessHreDw6uC0A8/78XnT1ZPB3wO35Sqjs888CGavb9ptdujNI6FlCU8CtfVqGvePtf4b7EwUGY7WPA1/e0Qt5+yKP1KTUMe6WHpi6qiAm1R7JUD1W1r1wZWcMfnWm7ff1a5OJKSsLQraf0cB4HeGGHtw5Mfg7UKtqumejF6lsaFq7imf19uS9diaLvkfy2jVdsOPAcfR/YUbA9tMbGE/DEWmNAjew2oc894C2mL3dRdLJPWxvcYd/zMEVEcZIVKqQ5uoC9G5gyZ88d1nXJrisa9mct5/Ir1+bTBw+UQjsPhLSoFsWlL0cExElgA9u7h55Jwt6ujDQLxoM/kREcfTxraGT1HENXyKiBGMUmJ3Eai/m8THCOn+iFMT23ujN+sd5KDh4PN7ZcIzBn4jIhkY1K9ue0M4udvUkothgX09XnN/WNxI31j8GseAo+ItIbRGZIiLrtP9rhdk3Q0S2ich/naRJRJQo7uzXCoseH4hGNUtH5M5/dAC+/cs5ccyVNU5L/qMATFNKtQYwTXtu5mkAvzpMj4hccG0P37z4fVpbX1OYQokIalVND5grKbN6RZzZtKbD4zrNWWRO6/yHAeivPf4AwAwA/wjeSUS6AagPYDKAbIdpEpFDV2U3xVXZTeOdjaTTvE6VyDslCKfBv75SKh8AlFL5IhIyFZ2IlAPwIoDrAZwf7mAiMhLASABo1ix+K/YQEUWjc5PwJf63b8hGlYqJsa51xOAvIlMBGC3Z86jFNO4CMEkptTXSNLdKqbEAxgJAdna2B+3dRETeGdCuvqX9vOjtEzH4K6UGmL0mIgUi0lAr9TcEsNNgt7MB9BGRuwBUA5AuIoeVUuHaB4iIypyy1InKabXPBAAjAIzW/v8ueAel1LX+xyJyI4BsBn4iSkZuldjLwvQOowEMFJF1AAZqzyEi2SLyttPMERGVBWWpxO/nqOSvlNoDg0ZcpVQOgFsNtr8P4H0naRIRkXMc4UtE5JKy1EuFwZ+IKIZaJNgKXn4M/kREMTT9wf6239Mggwu4ExElvL6tM9G4ZmXc1b+Vo+P8v6HtMaJXljuZioDBn4jIoVpV0zFr1HmOj+NV4AdY7UNElJIY/ImIUhCDPxFRCmLwJyJKQQz+REQpiMGfiCjObu/XEu0bZXiaJrt6EhHF2cOD2gKDvE2TJX8iohTE4E9ElIIY/ImIUhCDPxFRCmLwJyJKQQz+REQpiMGfiCgFMfgTEaUgBn8iohTE4E9ElIIY/ImIUhDn9iEiioEf/tobCzbvi3c2TDH4ExHFQIfGNdChcY14Z8MUq32IiFIQgz8RUQpi8CciSkEM/kREKYjBn4goBTkK/iJSW0SmiMg67f9aJvs1E5GfRWSViKwUkSwn6RIRkTNOS/6jAExTSrUGME17buRDAGOUUm0BdAew02G6RETkgNPgPwzAB9rjDwBcGryDiLQDUF4pNQUAlFKHlVJHHaZLREQOOB3kVV8plQ8ASql8EalnsE8bAPtF5GsALQBMBTBKKVUUvKOIjAQwUnt6WETWOMhbXQC7Hbw/FhIxTwDzZUci5glIzHwlYp6A5M9Xcys7RQz+IjIVQAODlx61mJHyAPoA6AJgC4DPANwI4J3gHZVSYwGMtXjcsEQkRymV7cax3JKIeQKYLzsSMU9AYuYrEfMEMF9+EYO/UmqA2WsiUiAiDbVSf0MY1+XnAViklNqovedbAD1hEPyJiMgbTuv8JwAYoT0eAeA7g33mA6glIpna8/MArHSYLhEROeA0+I8GMFBE1gEYqD2HiGSLyNsAoNXtPwhgmogsAyAA/ucwXStcqT5yWSLmCWC+7EjEPAGJma9EzBPAfAEARCnlZXpERJQAOMKXiCgFJV3wF5GLRGSNiKwXEbNBZ26l1VREpmsjl1eIyL3a9idFZJuILNb+Dda952Etb2tE5MJY5VtEckVkmZZ+jrbNcES2+Lyqpb1URLrqjjNC23+diIwwS89ink7XnZPFInJQRO6Lx/kSkXdFZKeILNdtc+38iEg37fyv194rUeZpjIis1tL9RkRqatuzROSY7py9GSlts78vyny59pmJSAsRmavl6zMRSY8yT5/p8pMrIovjcK7MYkJcry1DSqmk+QcgDcAGAC0BpANYAqBdDNNrCKCr9rg6gLUA2gF4EsCDBvu30/JUEb4xDxu0PLuebwC5AOoGbXsevjEWgG809nPa48EAfoSvPaYngLna9toANmr/19Ie13Lxs9oBX59kz88XgL4AugJYHovzA2AegLO19/wIYFCUeboAvkGSAPCcLk9Z+v2CjmOYttnfF2W+XPvMAHwOYLj2+E0Ad0aTp6DXXwTwRBzOlVlMiOu1ZfQv2Ur+3QGsV0ptVEqdBDAevlHIMaGUyldKLdQeHwKwCkDjMG8ZBmC8UuqEUmoTgPVanr3Kt9mI7GEAPlQ+cwDUFF/X3QsBTFFK7VVK7QMwBcBFLuXlfAAblFKbI+Q3JudLKfUbgL0G6Tk+P9prGUqp2cr3bf0QBqPfreRJKfWzUqpQezoHQJNwx4iQdsQR+VbzFYatz0wrtZ4H4Es7+QqXJ+2YVwH4NNwxYnSuzGJCXK8tI8kW/BsD2Kp7nofwwdg14pusrguAudqmu7XbuHd1t4xm+YtFvhWAn0VkgfhGTgNBI7IB+Edke5kvv+EI/HLG+3wB7p2fxtpjt/N3M3wlPb8WIrJIRH4VkT66vJqlbfb3RcuNz6wOgP26Hzg3zlUfAAVKqXW6bZ6fq6CYkHDXVrIFf6O6r5h3ZxKRagC+AnCfUuoggDcAtAJwJoB8+G5Bw+UvFvk+RynVFcAgAH8Rkb5h9vUyX9DqdIcC+ELblAjnKxy7+XA9fyLyKIBCAB9rm/IBNFNKdQFwP4BPRCQjFmmbcOszi0V+r0FgwcLzc2UQE0x3NclDzM9XsgX/PABNdc+bANgeywRFpAJ8H/LHSqmvAUApVaCUKlJKFcM3pqF7hPy5nm+l1Hbt/50AvtHyUKDdNvpvef0jsj3Ll2YQgIVKqQItj3E/Xxq3zk8eAqtnHOVPa+y7GMC12q0+tGqVPdrjBfDVp7eJkLbZ32ebi5/ZbviqOsoHbY+KdpzL4JtGxp9XT8+VUUwIc7z4XVvRNBQk6j/4pqvYCF9Dk79RqX0M0xP46txeDtreUPf4b/DVgQJAewQ2hm2EryHM1XwDqAqguu7xH/DV1Y9BYKPT89rjIQhsdJqnShudNsHX4FRLe1zbhfM2HsBN8T5fCGoIdPP8wDeyvSdKG+UGR5mni+AbEZ8ZtF8mgDTtcUsA2yKlbfb3RZkv1z4z+O4A9Q2+d0WTJ935+jVe5wrmMSHu11ZIXp1+kRPtH3yt52vh+3V/NMZp9YbvlmspgMXav8EAxgFYpm2fEPRFeVTL2xroWundzLd2gS/R/q3wHw+++tVpANZp//svJgHwupb2MgDZumPdDF+j3XroAraDvFUBsAdADd02z88XfNUC+QBOwVeausXN8wMgG8By7T3/hTagMoo8rYev7td/fb2p7Xu59tkuAbAQwCWR0jb7+6LMl2ufmXa9ztP+1i8AVIwmT9r29wHcEbSvl+fKLCbE9doy+scRvkREKSjZ6vyJiMgCBn8iohTE4E9ElIIY/ImIUhCDPxFRCmLwJyJKQQz+REQpiMGfiCgF/X8S1V3FI3aE5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you can skip this if you do not have matplotlib installed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# show 1 sec wave segment\n",
    "plt.plot(wav_segment)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show Mel spectrogram\n",
    "fig = plt.imshow(spectrogram, origin='lower')\n",
    "fig.set_cmap('jet')\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 1 big array of list of spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtype: convert the input data to the right data type used by Keras Deep Learning (GPU) (float32)\n",
    "dtype = floatx()\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 40, 80)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of many 40x80 spectrograms is made into 1 big array\n",
    "data = np.array(list_spectrograms, dtype=dtype)\n",
    "data_shape = data.shape\n",
    "data_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "We use <b>Zero-mean Unit-variance standardization</b> (also known as Z-score normalization).\n",
    "Here, we use <b>attribute-wise standardization</b>, i.e. each pixel is standardized individually, as opposed to computing a single mean and single standard deviation of all values.\n",
    "\n",
    "('Flat' standardization would also be possible, but we have seen benefits of attribut-wise standardization in our experiments).\n",
    "\n",
    "We use the StandardScaler from the scikit-learn package for our purpose.\n",
    "As it works typically on vector data, we have to vectorize (i.e. reshape) our matrices first. After standardization we have to reshape back to the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 3200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize\n",
    "N, ydim, xdim = data.shape\n",
    "data = data.reshape(N, xdim*ydim)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "scaler = preprocessing.StandardScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.1095439, -1.2172654, -1.2183886, ..., -5.256482 , -5.2489576,\n",
       "        -5.1149335], dtype=float32),\n",
       " array([1.0662463, 1.0477929, 1.0343325, ..., 1.0074166, 0.9897123,\n",
       "        0.9291812], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show mean and standard deviation: two vectors with same length as data.shape[1]\n",
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape back to the original format\n",
    "data = data.reshape(data_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train & Test Set \n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%).\n",
    "\n",
    "Here we compare Random Split vs. Stratified Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_size = 0.25 # % portion of whole data set to keep for testing, i.e. 75% is used for training\n",
    "\n",
    "# Normal (random) split of data set into 2 parts\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set, train_classes, test_classes = train_test_split(data, classes_num, test_size=testset_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: Class 0: 49 Class 1: 47\n"
     ]
    }
   ],
   "source": [
    "# The two classes may be unbalanced\n",
    "print(\"Class Counts: Class 0:\", sum(train_classes==0), \"Class 1:\", sum(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN INDEX: [ 97 102   0  34  13  44  60  20  75  22   6  24  99 116  47  61  14 126\n",
      "  68 104  89  17  95   7  37   3  93   8  40   9  69 122  72 119  71  82\n",
      "  33  74  51  49  78  29  76  98   5  50 121  23  12 105  36  96  25  90\n",
      " 101  55  92  21  80   2 125  16  32 127 124  48  28 112  63 110  77 109\n",
      "  54  79  45  53  66 115  86 120  52  59  81 118 106  19  43  94  65  15\n",
      "  26  84  30 107  10  91]\n",
      "TEST INDEX: [  1  27  64 117  88  85  35  18  46 100 111  11  83 103  87   4  70  31\n",
      " 108  58  57 123  38  73  56 113  67  39 114  42  41  62]\n"
     ]
    }
   ],
   "source": [
    "# better: Stratified Split retains the class balance in both sets\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(data, classes_num)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes_num[train_index]\n",
    "    test_classes = classes_num[test_index]\n",
    "# Note: this for loop is only executed once, if n_iter==1 resp. n_splits==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 40, 80)\n",
      "(32, 40, 80)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "# Note: we will reshape the data later back to matrix form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: Class 0: 48 Class 1: 48\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Counts: Class 0:\", sum(train_classes==0), \"Class 1:\", sum(train_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images or spectrograms, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which aggregates neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "### Adding the channel\n",
    "\n",
    "As CNNs were initially made for image data, we need to add a dimension for the color channel to the data. RGB images typically have a 3rd dimension with the color. \n",
    "\n",
    "<b>Spectrograms, however, are considered like greyscale images, as in the previous tutorial.\n",
    "Likewise we need to add an extra dimension for compatibility with the CNN implementation.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify image data format (Tensorflow: channels_last) \n",
    "#keras.backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 40, 80)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 40, 80)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1\n",
    "N, height, width = train_set.shape\n",
    "train_set = train_set.reshape(N, height, width, n_channels)  # Tensorflow ordering: channel last\n",
    "N, height, width = test_set.shape\n",
    "test_set = test_set.reshape(N, height, width, n_channels)  # height and width must be the same shape as train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 40, 80, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 40, 80, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 80, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "input_shape = train_set.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network Models in Keras\n",
    "\n",
    "## Sequential Models\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Single Layer and a Two Layer CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try: (comment/uncomment code in the following code block)\n",
    "* 1 Layer\n",
    "* 2 Layer\n",
    "* more conv_filters\n",
    "* Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(0) # make results repeatable\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "conv_filters = 32   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(conv_filters, (3, 3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# Layer 2\n",
    "#model.add(Conv2D(conv_filters, (3, 3)))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256, activation='sigmoid')) \n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 38, 78, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 39, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 39, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23712)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6070528   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,071,105\n",
      "Trainable params: 6,071,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.7308 - acc: 0.5000\n",
      "Epoch 2/15\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5760 - acc: 0.6875\n",
      "Epoch 3/15\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5170 - acc: 0.7500\n",
      "Epoch 4/15\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4852 - acc: 0.7812\n",
      "Epoch 5/15\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4653 - acc: 0.7917\n",
      "Epoch 6/15\n",
      "96/96 [==============================] - 0s 996us/step - loss: 0.4423 - acc: 0.7812\n",
      "Epoch 7/15\n",
      "96/96 [==============================] - 0s 998us/step - loss: 0.4239 - acc: 0.8021\n",
      "Epoch 8/15\n",
      "96/96 [==============================] - 0s 990us/step - loss: 0.4117 - acc: 0.8125\n",
      "Epoch 9/15\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3988 - acc: 0.8333\n",
      "Epoch 10/15\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3889 - acc: 0.8438\n",
      "Epoch 11/15\n",
      "96/96 [==============================] - 0s 995us/step - loss: 0.3817 - acc: 0.8438\n",
      "Epoch 12/15\n",
      "96/96 [==============================] - 0s 987us/step - loss: 0.3701 - acc: 0.8229\n",
      "Epoch 13/15\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3623 - acc: 0.8438\n",
      "Epoch 14/15\n",
      "96/96 [==============================] - 0s 985us/step - loss: 0.3501 - acc: 0.8438\n",
      "Epoch 15/15\n",
      "96/96 [==============================] - 0s 999us/step - loss: 0.3507 - acc: 0.8229\n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy goes up pretty quickly for 1 layer on Train set! Also on Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# always execute this, and then a box of accuracy_score below to print the result\n",
    "test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 layer\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layer\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layer + 32 convolution filters\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layer + 32 convolution filters + Dropout\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Parameters & Techniques\n",
    "\n",
    "Try out more parameters and techniques: (comment/uncomment code blocks below)\n",
    "* Adding ReLU activation\n",
    "* Adding Batch normalization\n",
    "* Adding Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "filter_size = (3,3)\n",
    "pool_size = (2,2)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(conv_filters, filter_size, padding='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=pool_size)) \n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(conv_filters, filter_size, padding='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=pool_size)) \n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256))  \n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and training the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.5505 - acc: 0.7604\n",
      "Epoch 2/15\n",
      "96/96 [==============================] - 0s 576us/step - loss: 0.4407 - acc: 0.7708\n",
      "Epoch 3/15\n",
      "96/96 [==============================] - 0s 592us/step - loss: 0.4096 - acc: 0.8542\n",
      "Epoch 4/15\n",
      "96/96 [==============================] - 0s 595us/step - loss: 0.3725 - acc: 0.8542\n",
      "Epoch 5/15\n",
      "96/96 [==============================] - 0s 589us/step - loss: 0.3441 - acc: 0.8646\n",
      "Epoch 6/15\n",
      "96/96 [==============================] - 0s 559us/step - loss: 0.3216 - acc: 0.8854\n",
      "Epoch 7/15\n",
      "96/96 [==============================] - 0s 569us/step - loss: 0.3044 - acc: 0.9062\n",
      "Epoch 8/15\n",
      "96/96 [==============================] - 0s 584us/step - loss: 0.2887 - acc: 0.8958\n",
      "Epoch 9/15\n",
      "96/96 [==============================] - 0s 573us/step - loss: 0.2747 - acc: 0.9062\n",
      "Epoch 10/15\n",
      "96/96 [==============================] - 0s 572us/step - loss: 0.2643 - acc: 0.8958\n",
      "Epoch 11/15\n",
      "96/96 [==============================] - 0s 569us/step - loss: 0.2542 - acc: 0.9271\n",
      "Epoch 12/15\n",
      "96/96 [==============================] - 0s 572us/step - loss: 0.2448 - acc: 0.9271\n",
      "Epoch 13/15\n",
      "96/96 [==============================] - 0s 579us/step - loss: 0.2364 - acc: 0.9167\n",
      "Epoch 14/15\n",
      "96/96 [==============================] - 0s 574us/step - loss: 0.2243 - acc: 0.9271\n",
      "Epoch 15/15\n",
      "96/96 [==============================] - 0s 583us/step - loss: 0.2185 - acc: 0.9271\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying Accuracy on Test Set\n",
    "test_pred = model.predict_classes(test_set)\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIREX 2015 Winning Model\n",
    "\n",
    "The following model was the winnner of the [MIREX 2015 Music/Speech Classification](https://www.music-ir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection_Results) benchmarking competition.\n",
    "(It was trained for 150 epochs on another data set though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIREX 2015 model\n",
    "model = Sequential()\n",
    "\n",
    "conv_filters = 15   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(conv_filters, (12, 8), padding='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) \n",
    "#model.add(Activation('sigmoid'))  # used in original model\n",
    "model.add(MaxPooling2D(pool_size=(2, 1))) \n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(200, activation='sigmoid'))  \n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))  # was not in original model\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and training the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.7202 - acc: 0.5000\n",
      "Epoch 2/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.6415 - acc: 0.6458\n",
      "Epoch 3/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5861 - acc: 0.6667\n",
      "Epoch 4/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5511 - acc: 0.7396\n",
      "Epoch 5/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5218 - acc: 0.7604\n",
      "Epoch 6/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5031 - acc: 0.7604\n",
      "Epoch 7/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4810 - acc: 0.7812\n",
      "Epoch 8/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4675 - acc: 0.7917\n",
      "Epoch 9/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4505 - acc: 0.7708\n",
      "Epoch 10/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4323 - acc: 0.8021\n",
      "Epoch 11/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4133 - acc: 0.8125\n",
      "Epoch 12/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3980 - acc: 0.8125\n",
      "Epoch 13/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3854 - acc: 0.8229\n",
      "Epoch 14/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3726 - acc: 0.8229\n",
      "Epoch 15/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3624 - acc: 0.8333\n",
      "Epoch 16/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3479 - acc: 0.8437\n",
      "Epoch 17/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3334 - acc: 0.8542\n",
      "Epoch 18/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3230 - acc: 0.8750\n",
      "Epoch 19/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3146 - acc: 0.8958\n",
      "Epoch 20/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3073 - acc: 0.8958\n",
      "Epoch 21/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2968 - acc: 0.8958\n",
      "Epoch 22/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2851 - acc: 0.9062\n",
      "Epoch 23/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2746 - acc: 0.9167\n",
      "Epoch 24/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2686 - acc: 0.9271\n",
      "Epoch 25/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2604 - acc: 0.9271\n",
      "Epoch 26/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2548 - acc: 0.9271\n",
      "Epoch 27/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2480 - acc: 0.9271\n",
      "Epoch 28/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2447 - acc: 0.9271\n",
      "Epoch 29/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2395 - acc: 0.9167\n",
      "Epoch 30/30\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2326 - acc: 0.9167\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "history = model.fit(train_set, train_classes, batch_size=40, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78125"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying Accuracy on Test Set\n",
    "test_pred = model.predict_classes(test_set)\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parallel CNNs: Graph Model\n",
    "\n",
    "These are graph models with more than 1 pipeline of Convolution layers: They use **2 parallel pipelines of Convolution and Pooling**, processing the same input data in different ways.\n",
    "\n",
    "This makes use of domain knowledge, where it is known in the Music IR domain that time and frequencies comprise different information in a Machine Learning task, which can be beneficial to process separately (depending on the use case). This is a major difference to processing images.\n",
    "\n",
    "It has been discovered, that **CNNs for music work better**, when they have one filter that is detecting frequencies in the vertical axis, and another filter that is focused on the time axis, i.e. detecting rhythm. Consequently, this is realized in a parallel CNN, where 2 layers are not stacked after each other, but first run independently in parallel with their output being merged later.\n",
    "\n",
    "Note that this might not hold true for using deeper networks.\n",
    "\n",
    "To create parallel CNNs we need a \"graph-based\" model. In Keras this is realized via the **functional API** of the **Model() class**.\n",
    "We use it to create two CNN layers that run in parallel to each other and are merged subsequently.\n",
    "In the functional API, you pass the name of the previous layer in (brackets) after defining the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET DESIRED PARAMETERS\n",
    "n_filters = 64 #16\n",
    "activation = 'relu'  # 'sigmoid'  # or other\n",
    "\n",
    "# MODEL DEFINITION\n",
    "\n",
    "# Input \"layer\" only specifies the input shape\n",
    "input = Input(input_shape)\n",
    "\n",
    "# CNN layers\n",
    "# The functional API allows to specify the predecessor in (brackets) after the new Layer function call\n",
    "\n",
    "# Pipeline 1\n",
    "conv_layer1 = Conv2D(n_filters, (10, 2), activation=activation)(input)       # vertical filter\n",
    "maxpool1 = MaxPooling2D(pool_size=(1,2))(conv_layer1) # horizontal pooling\n",
    "\n",
    "# Pipeline 2 (starting with same input)\n",
    "conv_layer2 = Conv2D(n_filters, (2, 10), activation=activation)(input)       # horizontal filter\n",
    "maxpool2 = MaxPooling2D(pool_size=(2,1))(conv_layer2) # vertical pooling\n",
    "\n",
    "# we have to flatten the Pooling output of both pipelines in order to be concatenated\n",
    "flat1 = Flatten()(maxpool1)\n",
    "flat2 = Flatten()(maxpool2)\n",
    "\n",
    "# Then we can merge (concatenate) the 2\n",
    "merged = concatenate([flat1, flat2])\n",
    "\n",
    "# we add a Dense layer after the merging\n",
    "full = Dense(256, activation=activation)(merged)\n",
    "\n",
    "# and the Output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(full)\n",
    "\n",
    "# finally create the model: define input and output layer\n",
    "model = Model(inputs=input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 80, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 31, 79, 64)   1344        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 39, 71, 64)   1344        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 31, 39, 64)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 19, 71, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 77376)        0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 86336)        0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 163712)       0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          41910528    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            257         dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 41,913,473\n",
      "Trainable params: 41,913,473\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "96/96 [==============================] - 3s 27ms/step - loss: 0.5803 - acc: 0.6354\n",
      "Epoch 2/15\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.3932 - acc: 0.8229\n",
      "Epoch 3/15\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.2994 - acc: 0.8958\n",
      "Epoch 4/15\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.3044 - acc: 0.8646\n",
      "Epoch 5/15\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.2254 - acc: 0.9375\n",
      "Epoch 6/15\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.2059 - acc: 0.9271\n",
      "Epoch 7/15\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.1835 - acc: 0.9583\n",
      "Epoch 8/15\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.1613 - acc: 0.9479\n",
      "Epoch 9/15\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.1226 - acc: 0.9896\n",
      "Epoch 10/15\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.1114 - acc: 0.9896\n",
      "Epoch 11/15\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1053 - acc: 0.9896\n",
      "Epoch 12/15\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1045 - acc: 0.9896\n",
      "Epoch 13/15\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0822 - acc: 0.9896\n",
      "Epoch 14/15\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.0782 - acc: 0.9896\n",
      "Epoch 15/15\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.0715 - acc: 0.9896\n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set\n",
    "\n",
    "Note: The functional API, i.e. Model() does not have a convenience method `.predict_classes()`. We therefore do 'raw' predictions with `predict()`, which returns values between 0 and 1, and then round to the nearest value (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_set)\n",
    "#test_pred[0:35,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = np.round(test_pred)\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Exercises 2\n",
    "\n",
    "* **Exercise 2.1)** Adjust the model defined under \"Additional Parameters & Techniques\" adapting Dropout and using different activation functions such as ReLU, Leaky ReLu, PRELU, ELU or SELU. \n",
    "* **Exercise 2.2)** Adjust the model defined under \"Parallel CNNs: Graph Model\" experimenting with different filter shapes in each pipeline, adding additional Conv layers per pipeline, adding a 3rd pipeline (and optionally experimenting with n_filters, activation, and Dropout).\n",
    "\n",
    "For both exercises:\n",
    "* try to improve the accuaracy on the test set\n",
    "* submit the adapted code including parameters, and the achieved accuaracy score on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
