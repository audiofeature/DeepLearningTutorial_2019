{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Deep Learning using Convolutional Neural Networks\n",
    "# Example: Car Recognition\n",
    "\n",
    "This tutorial shows how Convolutional Neural Networks are used to recognize cars on images vs. images with no cars on them (binary classification).\n",
    "\n",
    "The data set used is the [UIUC Image Database for Car Detection](http://cogcomp.cs.illinois.edu/Data/Car/) containing:\n",
    "* 1050 training images (550 car and 500 non-car images)\n",
    "* 170 test images, containing 200 cars at roughly the same scale as in the training images \n",
    "(we do not use the multi-scale test images, containing 139 cars at various scales, here)\n",
    "\n",
    "This tutorial contains:\n",
    "* Image Loading and Preprocessing\n",
    "* Standardization of Data\n",
    "* Fully Connected Neural Networks\n",
    "* Convolutional Neural Networks\n",
    "* Batch Normalization\n",
    "* ReLU Activation\n",
    "* Dropout\n",
    "* Data Augmentation\n",
    "* Plotting the Training Curve\n",
    "\n",
    "You can execute the following code blocks by pressing SHIFT+Enter consecutively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# if you want to use the GPU\n",
    "#device = 'gpu'\n",
    "#os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=' + device + ',floatX=float32'\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#import keras\n",
    "from keras.backend import floatx  # abstract default float type for Keras (e.g. float32)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "np.random.seed(1) # we initialize a random seed here to make the experiments repeatable with same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Images from Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the DATA_PATH please:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you use the iDSDL Lab machine\n",
    "DATA_PATH = '/home/ffg/idsdlmteacher1/Code/DeepLearningTutorial_2019/data'\n",
    "# if you downloaded the dataset into the tutorial folder (otherwise adjust the DATA_PATH please)\n",
    "# DATA_PATH = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 files\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(DATA_PATH, 'CarData/TrainImages')\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print(\"Found %d files\" % len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "image_names = []\n",
    "\n",
    "for filename in files:\n",
    "    image_names.append(os.path.basename(filename))\n",
    "    with Image.open(filename) as img:\n",
    "        images.append(np.array(img)) # we convert the images to a Numpy array and store them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos-372.pgm\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7F4541A57898>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show a selected image to check\n",
    "i=700\n",
    "print(image_names[i])\n",
    "Image.fromarray(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[i].shape   # height x width   (Numpy ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Color RGB images have an additional dimension of depth 3, e.g. (40, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 1 big array of list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of many 40x100 images is made into 1 big array\n",
    "# as data type we use \"floatx()\" from Keras which is imported above (for GPU it is most often float32)\n",
    "img_array = np.array(images, dtype=floatx())\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Groundtruth based on filenames:\n",
    "\n",
    "In this data set, images with cars start with \"pos-\" and images with no cars start with \"neg-\". We create a numeric list here, containing 1 for car images and 0 for non-car images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for name in image_names:\n",
    "    if name.startswith('neg'):\n",
    "        classes.append(0)\n",
    "    else:\n",
    "        classes.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 25 classes\n",
    "classes[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look classes 490 to 510\n",
    "classes[490:510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth Statistics:\n",
      "Class 0: 500 instances\n",
      "Class 1: 550 instances\n"
     ]
    }
   ],
   "source": [
    "print(\"Groundtruth Statistics:\")\n",
    "\n",
    "for c in set(classes):\n",
    "    print(\"Class {}: {} instances\".format(c, classes.count(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline = 0.524\n"
     ]
    }
   ],
   "source": [
    "baseline = 550 * 1.0 / len(classes)\n",
    "print(\"baseline = %.3f\" % baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline is 52.4%, i.e. a 'dumb' classifier can assign all predictions to the majority class achieving this accurcay. We aim at building a classifier that performs better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "Here we use <b>Zero-mean Unit-variance standardization</b> which means we deduct the mean and divide by the standard deviation.\n",
    "\n",
    "(Note: Here, we do this \"flat\", i.e. one mean and std.dev. for the whole image is computed over all pixels (not per pixel); in RGB images, standardization can be done e.g. for each colour channel individually; in other/non-image data sets, attribute-wise standardization should be applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 255.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.min(), img_array.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129.58241, 74.27671)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute mean and standard deviation\n",
    "mean = img_array.mean()\n",
    "stddev = img_array.std()\n",
    "mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.446289e-07, 0.9999999)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply zero mean unit variance standardization on all pixels of all images\n",
    "img_array = (img_array - mean) / stddev\n",
    "img_array.mean(), img_array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.74459, 7.446289e-07, 1.6885183)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the new min, mean and max\n",
    "img_array.min(), img_array.mean(), img_array.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network Models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Networks\n",
    "\n",
    "For a fully connected neural network, the x and y axis of an image do not play a role at all. All pixels are considered as a completely individual input to the neural network. Therefore the 2D image arrays have to be flattened to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 4000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  flatten images to vectors\n",
    "images_flat = img_array.reshape(img_array.shape[0],-1)\n",
    "images_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out input shape for NN, which is just a long vector (40x100 = 4000)\n",
    "input_shape = images_flat.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "In Keras, one can choose between a **Sequential model** and a **Graph model**. Sequential models are the standard case. Graph models are for parallel networks and use the functional API (see Music/Speech tutorial).\n",
    "\n",
    "Here we create a sequential model with 2 fully connected (a.k.a. 'dense') layers containing 256 units each.\n",
    "\n",
    "The output unit is a single Sigmoid unit which can predict values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple Fully-connected network with 2 hidden Dense layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_dim=input_shape, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the model's summary of layers is always a good idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               1024256   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,090,305\n",
      "Trainable params: 1,090,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function and Optimizer Strategy: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model: creates the whole model structure in memory \n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the input dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the number of epochs (= training cycles) and a batch size (= number of images presented to the network for each weight update):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 0s 409us/step - loss: 0.6762 - acc: 0.6362\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 0s 170us/step - loss: 0.6216 - acc: 0.7743\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 0s 171us/step - loss: 0.5798 - acc: 0.8600\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 0s 169us/step - loss: 0.5402 - acc: 0.8914\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 0s 171us/step - loss: 0.5026 - acc: 0.8933\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 0s 167us/step - loss: 0.4649 - acc: 0.9086\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 0s 167us/step - loss: 0.4302 - acc: 0.9076\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 0s 168us/step - loss: 0.3989 - acc: 0.9124\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 0s 167us/step - loss: 0.3700 - acc: 0.9219\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 0s 169us/step - loss: 0.3440 - acc: 0.9248\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 0s 169us/step - loss: 0.3211 - acc: 0.9276\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 0s 170us/step - loss: 0.3006 - acc: 0.9333\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 0s 170us/step - loss: 0.2825 - acc: 0.9324\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 0s 169us/step - loss: 0.2661 - acc: 0.9371\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 0s 169us/step - loss: 0.2520 - acc: 0.9381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4541a25668>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model.fit(images_flat, classes, batch_size=32, epochs=epochs) #, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify Accuracy on Train set\n",
    "predictions = model.predict_classes(images_flat)\n",
    "accuracy_score(classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% Accuracy - perfect, no?\n",
    "\n",
    "This is the accuracy on the training set. A (large, especially fully connected network with sufficient number of units) can easily learn the entire training set (especially a small one like here).\n",
    "\n",
    "This very likely leads to <b>overfitting</b>. That's why we test on an independent test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 170 files\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(DATA_PATH, 'CarData/TestImages')\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print(\"Found %d files\" % len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images to predict might be of different shape and size. We are using a resize and crop function (defined in image_preprocessing.py) to bring them to the same format (40x100) as needed by our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: original: (266, 158) \t reshaped: (100, 40)\n",
      "Image size: original: (155, 106) \t reshaped: (100, 40)\n",
      "Image size: original: (302, 146) \t reshaped: (100, 40)\n",
      "Image size: original: (150, 98) \t reshaped: (100, 40)\n",
      "Image size: original: (240, 145) \t reshaped: (100, 40)\n",
      "Image size: original: (167, 110) \t reshaped: (100, 40)\n",
      "Image size: original: (170, 91) \t reshaped: (100, 40)\n",
      "Image size: original: (160, 109) \t reshaped: (100, 40)\n",
      "Image size: original: (144, 98) \t reshaped: (100, 40)\n",
      "Image size: original: (175, 90) \t reshaped: (100, 40)\n",
      "Image size: original: (246, 110) \t reshaped: (100, 40)\n",
      "Image size: original: (142, 83) \t reshaped: (100, 40)\n",
      "Image size: original: (183, 123) \t reshaped: (100, 40)\n",
      "Image size: original: (146, 89) \t reshaped: (100, 40)\n",
      "Image size: original: (155, 106) \t reshaped: (100, 40)\n",
      "Image size: original: (146, 94) \t reshaped: (100, 40)\n",
      "Image size: original: (175, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (235, 117) \t reshaped: (100, 40)\n",
      "Image size: original: (160, 95) \t reshaped: (100, 40)\n",
      "Image size: original: (219, 132) \t reshaped: (100, 40)\n",
      "Image size: original: (170, 98) \t reshaped: (100, 40)\n",
      "Image size: original: (140, 95) \t reshaped: (100, 40)\n",
      "Image size: original: (132, 106) \t reshaped: (100, 40)\n",
      "Image size: original: (225, 140) \t reshaped: (100, 40)\n",
      "Image size: original: (159, 111) \t reshaped: (100, 40)\n",
      "Image size: original: (160, 100) \t reshaped: (100, 40)\n",
      "Image size: original: (190, 123) \t reshaped: (100, 40)\n",
      "Image size: original: (205, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (261, 156) \t reshaped: (100, 40)\n",
      "Image size: original: (200, 136) \t reshaped: (100, 40)\n",
      "Image size: original: (165, 113) \t reshaped: (100, 40)\n",
      "Image size: original: (110, 75) \t reshaped: (100, 40)\n",
      "Image size: original: (174, 83) \t reshaped: (100, 40)\n",
      "Image size: original: (160, 109) \t reshaped: (100, 40)\n",
      "Image size: original: (159, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (186, 121) \t reshaped: (100, 40)\n",
      "Image size: original: (145, 99) \t reshaped: (100, 40)\n",
      "Image size: original: (159, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (124, 85) \t reshaped: (100, 40)\n",
      "Image size: original: (209, 143) \t reshaped: (100, 40)\n",
      "Image size: original: (160, 97) \t reshaped: (100, 40)\n",
      "Image size: original: (264, 145) \t reshaped: (100, 40)\n",
      "Image size: original: (190, 101) \t reshaped: (100, 40)\n",
      "Image size: original: (280, 138) \t reshaped: (100, 40)\n",
      "Image size: original: (180, 101) \t reshaped: (100, 40)\n",
      "Image size: original: (220, 124) \t reshaped: (100, 40)\n",
      "Image size: original: (229, 118) \t reshaped: (100, 40)\n",
      "Image size: original: (146, 110) \t reshaped: (100, 40)\n",
      "Image size: original: (157, 107) \t reshaped: (100, 40)\n",
      "Image size: original: (180, 123) \t reshaped: (100, 40)\n",
      "Image size: original: (150, 88) \t reshaped: (100, 40)\n",
      "Image size: original: (160, 109) \t reshaped: (100, 40)\n",
      "Image size: original: (212, 126) \t reshaped: (100, 40)\n",
      "Image size: original: (229, 112) \t reshaped: (100, 40)\n",
      "Image size: original: (173, 103) \t reshaped: (100, 40)\n",
      "Image size: original: (245, 157) \t reshaped: (100, 40)\n",
      "Image size: original: (159, 99) \t reshaped: (100, 40)\n",
      "Image size: original: (260, 195) \t reshaped: (100, 40)\n",
      "Image size: original: (126, 86) \t reshaped: (100, 40)\n",
      "Image size: original: (266, 140) \t reshaped: (100, 40)\n",
      "Image size: original: (229, 146) \t reshaped: (100, 40)\n",
      "Image size: original: (180, 135) \t reshaped: (100, 40)\n",
      "Image size: original: (175, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (138, 73) \t reshaped: (100, 40)\n",
      "Image size: original: (175, 108) \t reshaped: (100, 40)\n",
      "Image size: original: (217, 141) \t reshaped: (100, 40)\n",
      "Image size: original: (159, 112) \t reshaped: (100, 40)\n",
      "Image size: original: (145, 99) \t reshaped: (100, 40)\n",
      "Image size: original: (150, 102) \t reshaped: (100, 40)\n",
      "Image size: original: (177, 107) \t reshaped: (100, 40)\n",
      "Image size: original: (190, 130) \t reshaped: (100, 40)\n",
      "Image size: original: (135, 76) \t reshaped: (100, 40)\n",
      "Image size: original: (181, 98) \t reshaped: (100, 40)\n",
      "Image size: original: (168, 106) \t reshaped: (100, 40)\n",
      "Image size: original: (140, 88) \t reshaped: (100, 40)\n",
      "Image size: original: (169, 115) \t reshaped: (100, 40)\n",
      "Image size: original: (176, 120) \t reshaped: (100, 40)\n",
      "Image size: original: (226, 125) \t reshaped: (100, 40)\n",
      "Image size: original: (225, 143) \t reshaped: (100, 40)\n",
      "Image size: original: (125, 85) \t reshaped: (100, 40)\n",
      "Image size: original: (186, 116) \t reshaped: (100, 40)\n",
      "Image size: original: (149, 112) \t reshaped: (100, 40)\n",
      "Image size: original: (266, 147) \t reshaped: (100, 40)\n",
      "Image size: original: (235, 176) \t reshaped: (100, 40)\n",
      "Image size: original: (230, 140) \t reshaped: (100, 40)\n",
      "Image size: original: (120, 82) \t reshaped: (100, 40)\n",
      "Image size: original: (272, 138) \t reshaped: (100, 40)\n",
      "Image size: original: (168, 102) \t reshaped: (100, 40)\n",
      "Image size: original: (184, 108) \t reshaped: (100, 40)\n",
      "Image size: original: (155, 98) \t reshaped: (100, 40)\n",
      "Image size: original: (235, 143) \t reshaped: (100, 40)\n",
      "Image size: original: (260, 155) \t reshaped: (100, 40)\n",
      "Image size: original: (230, 157) \t reshaped: (100, 40)\n",
      "Image size: original: (153, 115) \t reshaped: (100, 40)\n",
      "Image size: original: (234, 126) \t reshaped: (100, 40)\n",
      "Image size: original: (204, 127) \t reshaped: (100, 40)\n",
      "Image size: original: (180, 100) \t reshaped: (100, 40)\n",
      "Image size: original: (205, 134) \t reshaped: (100, 40)\n",
      "Image size: original: (151, 98) \t reshaped: (100, 40)\n",
      "Image size: original: (189, 110) \t reshaped: (100, 40)\n",
      "Image size: original: (167, 112) \t reshaped: (100, 40)\n",
      "Image size: original: (152, 91) \t reshaped: (100, 40)\n",
      "Image size: original: (165, 102) \t reshaped: (100, 40)\n",
      "Image size: original: (213, 98) \t reshaped: (100, 40)\n",
      "Image size: original: (167, 91) \t reshaped: (100, 40)\n",
      "Image size: original: (302, 186) \t reshaped: (100, 40)\n",
      "Image size: original: (216, 159) \t reshaped: (100, 40)\n",
      "Image size: original: (160, 83) \t reshaped: (100, 40)\n",
      "Image size: original: (174, 96) \t reshaped: (100, 40)\n",
      "Image size: original: (235, 134) \t reshaped: (100, 40)\n",
      "Image size: original: (170, 116) \t reshaped: (100, 40)\n",
      "Image size: original: (250, 156) \t reshaped: (100, 40)\n",
      "Image size: original: (210, 115) \t reshaped: (100, 40)\n",
      "Image size: original: (167, 101) \t reshaped: (100, 40)\n",
      "Image size: original: (220, 145) \t reshaped: (100, 40)\n",
      "Image size: original: (178, 109) \t reshaped: (100, 40)\n",
      "Image size: original: (206, 136) \t reshaped: (100, 40)\n",
      "Image size: original: (253, 172) \t reshaped: (100, 40)\n",
      "Image size: original: (147, 100) \t reshaped: (100, 40)\n",
      "Image size: original: (252, 155) \t reshaped: (100, 40)\n",
      "Image size: original: (184, 124) \t reshaped: (100, 40)\n",
      "Image size: original: (325, 147) \t reshaped: (100, 40)\n",
      "Image size: original: (188, 112) \t reshaped: (100, 40)\n",
      "Image size: original: (270, 162) \t reshaped: (100, 40)\n",
      "Image size: original: (200, 150) \t reshaped: (100, 40)\n",
      "Image size: original: (126, 86) \t reshaped: (100, 40)\n",
      "Image size: original: (167, 99) \t reshaped: (100, 40)\n",
      "Image size: original: (225, 127) \t reshaped: (100, 40)\n",
      "Image size: original: (228, 128) \t reshaped: (100, 40)\n",
      "Image size: original: (163, 111) \t reshaped: (100, 40)\n",
      "Image size: original: (150, 90) \t reshaped: (100, 40)\n",
      "Image size: original: (128, 96) \t reshaped: (100, 40)\n",
      "Image size: original: (212, 128) \t reshaped: (100, 40)\n",
      "Image size: original: (172, 117) \t reshaped: (100, 40)\n",
      "Image size: original: (301, 179) \t reshaped: (100, 40)\n",
      "Image size: original: (138, 87) \t reshaped: (100, 40)\n",
      "Image size: original: (175, 108) \t reshaped: (100, 40)\n",
      "Image size: original: (150, 103) \t reshaped: (100, 40)\n",
      "Image size: original: (128, 77) \t reshaped: (100, 40)\n",
      "Image size: original: (178, 114) \t reshaped: (100, 40)\n",
      "Image size: original: (168, 115) \t reshaped: (100, 40)\n",
      "Image size: original: (180, 126) \t reshaped: (100, 40)\n",
      "Image size: original: (168, 115) \t reshaped: (100, 40)\n",
      "Image size: original: (179, 100) \t reshaped: (100, 40)\n",
      "Image size: original: (175, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (154, 105) \t reshaped: (100, 40)\n",
      "Image size: original: (200, 150) \t reshaped: (100, 40)\n",
      "Image size: original: (175, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (160, 104) \t reshaped: (100, 40)\n",
      "Image size: original: (207, 116) \t reshaped: (100, 40)\n",
      "Image size: original: (185, 126) \t reshaped: (100, 40)\n",
      "Image size: original: (220, 130) \t reshaped: (100, 40)\n",
      "Image size: original: (360, 176) \t reshaped: (100, 40)\n",
      "Image size: original: (200, 139) \t reshaped: (100, 40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: original: (200, 150) \t reshaped: (100, 40)\n",
      "Image size: original: (142, 92) \t reshaped: (100, 40)\n",
      "Image size: original: (135, 93) \t reshaped: (100, 40)\n",
      "Image size: original: (206, 137) \t reshaped: (100, 40)\n",
      "Image size: original: (175, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (277, 164) \t reshaped: (100, 40)\n",
      "Image size: original: (275, 137) \t reshaped: (100, 40)\n",
      "Image size: original: (200, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (265, 199) \t reshaped: (100, 40)\n",
      "Image size: original: (200, 113) \t reshaped: (100, 40)\n",
      "Image size: original: (194, 125) \t reshaped: (100, 40)\n",
      "Image size: original: (180, 116) \t reshaped: (100, 40)\n",
      "Image size: original: (149, 97) \t reshaped: (100, 40)\n",
      "Image size: original: (119, 90) \t reshaped: (100, 40)\n",
      "Image size: original: (188, 119) \t reshaped: (100, 40)\n",
      "Image size: original: (193, 124) \t reshaped: (100, 40)\n"
     ]
    }
   ],
   "source": [
    "from image_preprocessing import resize_and_crop\n",
    "\n",
    "test_images = []\n",
    "\n",
    "for filename in files:\n",
    "    with Image.open(filename) as img:\n",
    "        img_resized = resize_and_crop(img,target_width=100,target_height=40)\n",
    "        test_images.append(np.array(img_resized))\n",
    "        print(\"Image size: original: {} \\t reshaped: {}\".format(img.size, img_resized.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7F453FBEE518>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show random image\n",
    "i=4\n",
    "Image.fromarray(test_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 1 big array again from list\n",
    "test_images = np.array(test_images, dtype=floatx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Test Set\n",
    "\n",
    "The test data has to be standardized <b>in the same way</b> as the training data for compatibility with the model! That means, we take the mean and standard deviation of the <i>training data</i> to transform also the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO! we take the same mean and stddev from the training data above!\n",
    "#mean = test_images.mean()\n",
    "#stddev = test_images.std()\n",
    "#print(mean, stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122.19739, 72.07709)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.mean(), test_images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-use values from train set above\n",
    "test_images = (test_images - mean) / stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Images for Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 4000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_flat = test_images.reshape(test_images.shape[0],-1)\n",
    "test_images_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_images_flat)\n",
    "# show 30 first predictions\n",
    "test_pred[0:30,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groundtruth:\n",
    "# this TEST SET contains ONLY CARS on images! \n",
    "# Thus all the test classes are 1\n",
    "test_classes = [1] * len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's count the number of ones ...\n",
    "test_pred.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ALL our test classes are 1, counting the number of 1's and dividing by number of files gives us the Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1588235294117647"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.sum() / len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real way to do it is to compare the predictions (test_pred) with the ground truth (test_classes) and sum up the correct ones.\n",
    "This is exactly what the scikit-learn function <i>accuracy_score</i> does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1588235294117647"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(test_classes, test_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the Test Set is rather low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which groups neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Our input to the CNN is the standardized version of the original image array.\n",
    "\n",
    "#### Adding the channel\n",
    "\n",
    "For CNNs, we need to add a dimension for the color channel to the data. RGB images typically have an 3rd dimension with the color. \n",
    "<b>For greyscale images we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1\n",
    "N, height, width = img_array.shape\n",
    "train_img = img_array.reshape(N, height, width, n_channels)  # Tensorflow ordering: channnel last\n",
    "N, height, width = test_images.shape  # height and width should/must be the same as img_array\n",
    "test_img = test_images.reshape(N, height, width, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 2 alternative ways to do the same:\n",
    "# a) train_img = np.expand_dims(img_array, axis=3)\n",
    "# b) train_img = img_array[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 100, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "    \n",
    "input_shape = train_img.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple CNN model\n",
    "\n",
    "We start with a simple 2 layer Convolutional Neural Network, defined as a Keras Sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), input_shape=input_shape, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  # reducing image resolution by half\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))  # input_shape is only needed in 1st layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # the 2D output of a Conv Layer has to be flattened before going into a Dense Layer\n",
    "    model.add(Flatten()) # Note: Keras does automatic shape inference.\n",
    "\n",
    "    # Full Layer\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 38, 98, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 47, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 23, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5888)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                376896    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 381,761\n",
      "Trainable params: 381,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simpleModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "loss = 'binary_crossentropy' \n",
    "optimizer = 'sgd' \n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 1s 1ms/step - loss: 0.6910 - acc: 0.5543\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 1s 721us/step - loss: 0.6514 - acc: 0.6286\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 1s 757us/step - loss: 0.6123 - acc: 0.7771\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 1s 757us/step - loss: 0.5548 - acc: 0.8438\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 1s 737us/step - loss: 0.4824 - acc: 0.8610\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 1s 729us/step - loss: 0.4132 - acc: 0.8895\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 1s 723us/step - loss: 0.3549 - acc: 0.8990\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 1s 725us/step - loss: 0.3117 - acc: 0.9029\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 1s 718us/step - loss: 0.2826 - acc: 0.9114\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 1s 724us/step - loss: 0.2569 - acc: 0.9162\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 1s 719us/step - loss: 0.2334 - acc: 0.9238\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 1s 719us/step - loss: 0.2131 - acc: 0.9333\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 1s 730us/step - loss: 0.1940 - acc: 0.9429\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 1s 751us/step - loss: 0.1771 - acc: 0.9533\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 1s 740us/step - loss: 0.1620 - acc: 0.9581\n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_img, classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Again, our Accuracy rises quickly to almost 100%, with the CNN now even faster than with the Fully Connected Network.\n",
    "But is our model really good at predicting unseen data?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Validation Data\n",
    "\n",
    "We split off 10 % of the training data and use it as independend validation set to verify how good we are\n",
    "on an independent data (not used for training) in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945 samples, validate on 105 samples\n",
      "Epoch 1/15\n",
      "945/945 [==============================] - 1s 1ms/step - loss: 0.6891 - acc: 0.5608 - val_loss: 0.6688 - val_acc: 0.6190\n",
      "Epoch 2/15\n",
      "945/945 [==============================] - 1s 765us/step - loss: 0.6514 - acc: 0.6381 - val_loss: 0.6513 - val_acc: 0.8190\n",
      "Epoch 3/15\n",
      "945/945 [==============================] - 1s 752us/step - loss: 0.6244 - acc: 0.8296 - val_loss: 0.6194 - val_acc: 0.7048\n",
      "Epoch 4/15\n",
      "945/945 [==============================] - 1s 761us/step - loss: 0.5870 - acc: 0.7894 - val_loss: 0.5848 - val_acc: 0.8571\n",
      "Epoch 5/15\n",
      "945/945 [==============================] - 1s 757us/step - loss: 0.5371 - acc: 0.8995 - val_loss: 0.5338 - val_acc: 0.8667\n",
      "Epoch 6/15\n",
      "945/945 [==============================] - 1s 756us/step - loss: 0.4769 - acc: 0.8995 - val_loss: 0.4763 - val_acc: 0.8762\n",
      "Epoch 7/15\n",
      "945/945 [==============================] - 1s 758us/step - loss: 0.4118 - acc: 0.9090 - val_loss: 0.4198 - val_acc: 0.8952\n",
      "Epoch 8/15\n",
      "945/945 [==============================] - 1s 765us/step - loss: 0.3531 - acc: 0.9196 - val_loss: 0.3730 - val_acc: 0.8762\n",
      "Epoch 9/15\n",
      "945/945 [==============================] - 1s 752us/step - loss: 0.3068 - acc: 0.9153 - val_loss: 0.3372 - val_acc: 0.9048\n",
      "Epoch 10/15\n",
      "945/945 [==============================] - 1s 752us/step - loss: 0.2742 - acc: 0.9217 - val_loss: 0.3102 - val_acc: 0.9143\n",
      "Epoch 11/15\n",
      "945/945 [==============================] - 1s 760us/step - loss: 0.2469 - acc: 0.9291 - val_loss: 0.2782 - val_acc: 0.9238\n",
      "Epoch 12/15\n",
      "945/945 [==============================] - 1s 755us/step - loss: 0.2231 - acc: 0.9354 - val_loss: 0.2548 - val_acc: 0.9238\n",
      "Epoch 13/15\n",
      "945/945 [==============================] - 1s 764us/step - loss: 0.2029 - acc: 0.9397 - val_loss: 0.2335 - val_acc: 0.9238\n",
      "Epoch 14/15\n",
      "945/945 [==============================] - 1s 747us/step - loss: 0.1842 - acc: 0.9503 - val_loss: 0.2111 - val_acc: 0.9333\n",
      "Epoch 15/15\n",
      "945/945 [==============================] - 1s 752us/step - loss: 0.1678 - acc: 0.9534 - val_loss: 0.1931 - val_acc: 0.9429\n"
     ]
    }
   ],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = simpleModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# train with showing accuracy on split off validation data\n",
    "history = model.fit(train_img, classes, batch_size=32, epochs=epochs, validation_split=0.1) # portion of val. data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the split-off validation data are quite high (usually similar, but not as high as on the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Test Set as Validation Set\n",
    "\n",
    "<b>Note: This usually is not recommended as during experimentation you will overfit also to the test data.</b>\n",
    "\n",
    "We show it here only for demonstration purposes to see how (bad) the validation accuracy is on our independet test data. The recommended way is to have a separate training, validation and test set (i.e. 3 splits or separate data sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1050 samples, validate on 170 samples\n",
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 1s 1ms/step - loss: 0.6967 - acc: 0.5333 - val_loss: 0.6361 - val_acc: 0.9412\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 1s 778us/step - loss: 0.6556 - acc: 0.6400 - val_loss: 0.6766 - val_acc: 0.5647\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 1s 771us/step - loss: 0.6134 - acc: 0.7952 - val_loss: 0.6895 - val_acc: 0.4706\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 1s 778us/step - loss: 0.5612 - acc: 0.8333 - val_loss: 0.8912 - val_acc: 0.1353\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 1s 767us/step - loss: 0.4917 - acc: 0.8886 - val_loss: 1.0221 - val_acc: 0.1176\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 1s 782us/step - loss: 0.4177 - acc: 0.9048 - val_loss: 1.1996 - val_acc: 0.1118\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 1s 759us/step - loss: 0.3513 - acc: 0.9124 - val_loss: 1.4384 - val_acc: 0.1059\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 1s 779us/step - loss: 0.2969 - acc: 0.9229 - val_loss: 1.6691 - val_acc: 0.1059\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 1s 769us/step - loss: 0.2581 - acc: 0.9286 - val_loss: 1.6765 - val_acc: 0.1412\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 1s 768us/step - loss: 0.2254 - acc: 0.9381 - val_loss: 1.7660 - val_acc: 0.1471\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 1s 767us/step - loss: 0.2018 - acc: 0.9400 - val_loss: 1.9118 - val_acc: 0.1471\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 1s 762us/step - loss: 0.1812 - acc: 0.9410 - val_loss: 2.0423 - val_acc: 0.1353\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 1s 781us/step - loss: 0.1657 - acc: 0.9505 - val_loss: 2.0171 - val_acc: 0.1588\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 1s 770us/step - loss: 0.1507 - acc: 0.9552 - val_loss: 1.9424 - val_acc: 0.1765\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 1s 765us/step - loss: 0.1404 - acc: 0.9610 - val_loss: 1.8924 - val_acc: 0.1824\n"
     ]
    }
   ],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = simpleModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# show result on Test Data while training \n",
    "# we use test data as validation data to see direct results (usually NOT RECOMMENDED due to overfitting to the problem!)\n",
    "validation_data = (test_img, test_classes)\n",
    "\n",
    "history = model.fit(train_img, classes, batch_size=32, epochs=epochs, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>On the test set we perform really bad: less than 15% Accuracy!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "# show 35 first predictions\n",
    "test_pred[0:35,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_classes[0:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18235294117647058"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more advanced CNN model\n",
    "\n",
    "You may try to change the following to see the impact on the result:\n",
    "* number of filters\n",
    "* filter kernel size (e.g. 3 x 3, 5 x 5, ...)\n",
    "* adding/not adding Batch Normalization\n",
    "* adding/not adding ReLU Activation\n",
    "* adding/not adding Max Pooling\n",
    "* changing Pooling size (e.g. 1 x 2, 2 x 2, 2 x 1, or more)\n",
    "* adding/changing/removing Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advancedModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    # this applies n_filters convolution filters of size 3x3 in each of the 2 layers below\n",
    "    n_filters = 16\n",
    "    kernel_size = (3, 3)\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(n_filters, kernel_size, padding='valid', input_shape=input_shape))\n",
    "    # input shape: 100x100 images with 3 channels -> input_shape should be (3, 100, 100) \n",
    "    model.add(BatchNormalization())  # ON/OFF\n",
    "    model.add(Activation('relu'))  # ReLu activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) # reducing image resolution by half\n",
    "    model.add(Dropout(0.3))  # ON/OFF: random \"deletion\" of %-portion of units in each batch\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(n_filters, kernel_size))  # input_shape is only needed in 1st layer\n",
    "    #model.add(BatchNormalization()) # ON/OFF\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))  # ON/OFF\n",
    "    model.add(Dropout(0.3))  # ON/OFF\n",
    "\n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 38, 98, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 38, 98, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38, 98, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 19, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 17, 47, 16)        2320      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 17, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12784)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               3272960   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,275,761\n",
      "Trainable params: 3,275,729\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = advancedModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "loss = 'binary_crossentropy' \n",
    "optimizer = 'sgd' \n",
    "#optimizer = SGD(lr=0.001)  # possibility to adapt the learn rate\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 3s 3ms/step - loss: 0.5270 - acc: 0.8010\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.1915 - acc: 0.9219\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.1283 - acc: 0.9533\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0873 - acc: 0.9676\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0517 - acc: 0.9876\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0508 - acc: 0.9867\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0443 - acc: 0.9876\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0300 - acc: 0.9924\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0394 - acc: 0.9829\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0253 - acc: 0.9924A: 0s - loss: 0.0244 - acc: 0.992\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0198 - acc: 0.9943\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0184 - acc: 0.9962\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0250 - acc: 0.9914\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0144 - acc: 0.9962\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0186 - acc: 0.9952\n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_img, classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Training Curve\n",
    "\n",
    "The `model.fit` function returns a history including the evolution of training and validation loss and accuracy. We can plot it to see a nice training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = history.history\n",
    "hist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f435077e160>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this will only work if you have matplotlib installed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "colors = {'loss':'r', 'acc':'b', 'val_loss':'m', 'val_acc':'g'}\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Training Curve\") \n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "for measure in hist.keys():\n",
    "    color = colors[measure]\n",
    "    plt.plot(range(1,epochs+1), hist[measure], color + '-', label=measure)  # use last 2 values to draw line\n",
    "\n",
    "plt.legend(loc='upper left', scatterpoints = 1, frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: We are completely overfitting!\n",
    "While we achieve nearly 100% on the Training Set, the generalization to the Test Set is really bad, with an Accuracy of about only 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Increase the training set by adding more images: Rotate, shift, flip and scale the original images to generate additional examples that will help the Neural Network to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator needs the classes as Numpy array instead of normal list\n",
    "classes_array = np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # enforce repeatable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "steps_per_epoch = len(train_img) / batch_size\n",
    "\n",
    "int(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "33/32 [==============================] - 3s 101ms/step - loss: 0.6978 - acc: 0.5846 - val_loss: 0.5980 - val_acc: 0.6882\n",
      "Epoch 2/15\n",
      "33/32 [==============================] - 2s 71ms/step - loss: 0.5960 - acc: 0.6740 - val_loss: 0.7219 - val_acc: 0.5765\n",
      "Epoch 3/15\n",
      "33/32 [==============================] - 2s 71ms/step - loss: 0.5750 - acc: 0.7037 - val_loss: 0.8361 - val_acc: 0.4824\n",
      "Epoch 4/15\n",
      "33/32 [==============================] - 2s 72ms/step - loss: 0.5290 - acc: 0.7295 - val_loss: 1.3836 - val_acc: 0.1647\n",
      "Epoch 5/15\n",
      "33/32 [==============================] - 2s 70ms/step - loss: 0.5082 - acc: 0.7375 - val_loss: 0.8696 - val_acc: 0.4353\n",
      "Epoch 6/15\n",
      "33/32 [==============================] - 2s 70ms/step - loss: 0.4963 - acc: 0.7588 - val_loss: 0.8090 - val_acc: 0.4941\n",
      "Epoch 7/15\n",
      "33/32 [==============================] - 2s 72ms/step - loss: 0.4518 - acc: 0.7947 - val_loss: 0.5715 - val_acc: 0.6588\n",
      "Epoch 8/15\n",
      "33/32 [==============================] - 2s 72ms/step - loss: 0.4012 - acc: 0.8148 - val_loss: 1.7280 - val_acc: 0.1529\n",
      "Epoch 9/15\n",
      "33/32 [==============================] - 2s 69ms/step - loss: 0.3849 - acc: 0.8232 - val_loss: 0.9896 - val_acc: 0.4706\n",
      "Epoch 10/15\n",
      "32/32 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8448"
     ]
    }
   ],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = advancedModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "# steps_per_epoch is the number of batches to draw from the generator at each epoch. \n",
    "# suggestion: samples_per_epoch(= full train set)/batch_size\n",
    "steps_per_epoch = len(train_img) / batch_size\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit_generator(datagen.flow(train_img, classes_array, batch_size=batch_size),\n",
    "                    steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "                    validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set (with Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "test_pred[0:35,0] # show 35 first predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion: With Data Augmentation, the model has more diverse training examples and generalizes much better. The Accuracy on the Test Set increases from less than 10% to more than 55% up to 75%!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Training Curve with Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history.history\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Training Curve\") \n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "for measure in hist.keys():\n",
    "    color = colors[measure]\n",
    "    plt.plot(range(1,epochs+1), hist[measure], color + '-', label=measure)  # use last 2 values to draw line\n",
    "\n",
    "plt.legend(loc='upper left', scatterpoints = 1, frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is not ideal, but we see both training (blue) and validation (green) Accuracy going up, the training loss going continuously down and the validation loss fluctuating, but not exploding like previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Exercises 1\n",
    "\n",
    "* **Exercise 1.1)** Adjust the parameters mentioned under \"A more advanced CNN model\" in the code block of \"def advancedModel():\" above: Tune the parameters and find a model configuration that performs better on the Test Set.\n",
    "* **Exercise 1.2)** Try to improve the Data Augmentation further by adjusting the parameters of the ImageDataGenerator and/or the batch_size and/or steps_per_epoch.\n",
    "\n",
    "For both exercises include:\n",
    "* the code block with the parametrs of the model / augmentation you adapted\n",
    "* the best achieved validation accuracy + valiation loss\n",
    "* the plot of the training curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
